{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Copyright (C) 2019 SAMSUNG SDS <Team.SAIDA@gmail.com>\n",
    "#\n",
    "# This code is distribued under the terms and conditions from the MIT License (MIT).\n",
    "#\n",
    "# Authors : Uk Jo, Iljoo Yoon, Hyunjae Lee, Daehun Jun\n",
    "\n",
    "# Initial framework taken from https://github.com/jaara/AI-blog/blob/master/CartPole-A3C.py\n",
    "\n",
    "from core.algorithm.PPO import PPOAgent\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from core.common.processor import Processor\n",
    "from saida_gym.starcraft.avoidReavers import AvoidReavers\n",
    "from core.callbacks import DrawTrainMovingAvgPlotCallback\n",
    "import saida_gym.envs.conn.connection_env as Config\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper param\n",
    "NB_STEPS = 50000\n",
    "STATE_SIZE = 8 + 3 * 8\n",
    "LOSS_CLIPPING = 0.2  # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "EPOCHS = 10\n",
    "NOISE = 0.1  # Exploration noise\n",
    "GAMMA = .99\n",
    "BUFFER_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_SIZE = 80\n",
    "NUM_LAYERS = 3\n",
    "ENTROPY_LOSS = 1e-3\n",
    "LR = 1e-4  # Lower lr stabilises training greatly\n",
    "\n",
    "\n",
    "def scale_velocity(v):\n",
    "    return v\n",
    "\n",
    "\n",
    "def scale_angle(angle):\n",
    "    return (angle - math.pi) / math.pi\n",
    "\n",
    "\n",
    "def scale_pos(pos):\n",
    "    return pos / 16\n",
    "\n",
    "\n",
    "def scale_pos2(pos):\n",
    "    return pos / 8\n",
    "\n",
    "\n",
    "def exponential_average(old, new, b1):\n",
    "    return old * b1 + (1-b1) * new\n",
    "\n",
    "\n",
    "# Reshape the reward in a way you want\n",
    "def reward_reshape(reward):\n",
    "    \"\"\" Reshape the reward\n",
    "        Starcraft Env returns the reward according to following conditions.\n",
    "        1. Invalid action : -0.1\n",
    "        2. get hit : -1\n",
    "        3. goal : 1\n",
    "        4. others : 0\n",
    "\n",
    "    # Argument\n",
    "        reward (float): The observed reward after executing the action\n",
    "\n",
    "    # Returns\n",
    "        reshaped reward\n",
    "    \"\"\"\n",
    "\n",
    "    if math.fabs(reward + 0.1) < 0.01:\n",
    "        reward = -1\n",
    "    elif reward == -1:\n",
    "        reward = -10\n",
    "    elif reward == 1:\n",
    "        reward = 10\n",
    "    elif reward == 0:\n",
    "        reward = -0.1\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReaverProcessor(Processor):\n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "        self.success_cnt = 0\n",
    "        self.cumulate_reward = 0\n",
    "\n",
    "    def process_action(self, action):\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        state_array = self.process_observation(observation)\n",
    "        reward = reward_reshape(reward)\n",
    "        self.cumulate_reward += reward\n",
    "\n",
    "        if reward == 10:\n",
    "            if self.cumulate_reward > 0:\n",
    "                self.success_cnt += 1\n",
    "\n",
    "            self.cumulate_reward = 0\n",
    "            print(\"success_cnt = \", self.success_cnt)\n",
    "\n",
    "        return state_array, reward, done, info\n",
    "\n",
    "    def process_observation(self, observation, **kwargs):\n",
    "        \"\"\" Pre-process observation\n",
    "\n",
    "        # Argument\n",
    "            observation (object): The current observation from the environment.\n",
    "\n",
    "        # Returns\n",
    "            processed observation\n",
    "\n",
    "        \"\"\"\n",
    "        if len(observation.my_unit) > 0:\n",
    "            s = np.zeros(STATE_SIZE)\n",
    "            me = observation.my_unit[0]\n",
    "            # Observation for Dropship\n",
    "            s[0] = scale_pos2(me.pos_x)  # X of coordinates\n",
    "            s[1] = scale_pos2(me.pos_y)  # Y of coordinates\n",
    "            s[2] = scale_pos2(me.pos_x - 320)  # relative X of coordinates from goal\n",
    "            s[3] = scale_pos2(me.pos_y - 320)  # relative Y of coordinates from goal\n",
    "            s[4] = scale_velocity(me.velocity_x)  # X of velocity\n",
    "            s[5] = scale_velocity(me.velocity_y)  # y of coordinates\n",
    "            s[6] = scale_angle(me.angle)  # Angle of head of dropship\n",
    "            s[7] = 1 if me.accelerating else 0  # True if Dropship is accelerating\n",
    "\n",
    "            # Observation for Reavers\n",
    "            for ind, ob in enumerate(observation.en_unit):\n",
    "                s[ind * 8 + 8] = scale_pos2(ob.pos_x - me.pos_x)  # X of relative coordinates\n",
    "                s[ind * 8 + 9] = scale_pos2(ob.pos_y - me.pos_y)  # Y of relative coordinates\n",
    "                s[ind * 8 + 10] = scale_pos2(ob.pos_x - 320)  # X of relative coordinates\n",
    "                s[ind * 8 + 11] = scale_pos2(ob.pos_y - 320)  # Y of relative coordinates\n",
    "                s[ind * 8 + 12] = scale_velocity(ob.velocity_x)  # X of velocity\n",
    "                s[ind * 8 + 13] = scale_velocity(ob.velocity_y)  # Y of velocity\n",
    "                s[ind * 8 + 14] = scale_angle(ob.angle)  # Angle of head of Reavers\n",
    "                s[ind * 8 + 15] = 1 if ob.accelerating else 0  # True if Reaver is accelerating\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "def build_actor(state_size, action_size, advantage, old_prediction):\n",
    "    state_input = Input(shape=(state_size,))\n",
    "\n",
    "    x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "    for _ in range(NUM_LAYERS - 1):\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "    out_actions = Dense(action_size, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_actor_continuous(state_size, action_size, advantage, old_prediction):\n",
    "\n",
    "    state_input = Input(shape=(state_size,))\n",
    "    x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "\n",
    "    for _ in range(NUM_LAYERS - 1):\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "    out_actions = Dense(action_size, name='output', activation='tanh')(x)\n",
    "\n",
    "    model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_critic(state_size):\n",
    "\n",
    "    state_input = Input(shape=(state_size,))\n",
    "    x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "    for _ in range(NUM_LAYERS - 1):\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "    out_value = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[state_input], outputs=[out_value])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize...\n",
      "Shared Memory create\u0000\n",
      "SAIDA_AR19292 Shared memory found.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 80)                2640      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 80)                6480      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 37)                2997      \n",
      "=================================================================\n",
      "Total params: 18,597\n",
      "Trainable params: 18,597\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_weights() missing 1 required positional argument: 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d6b864127ea0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mcb_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDrawTrainMovingAvgPlotCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFILE_NAME\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'episode_reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../../../../'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'ppo'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNB_STEPS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcb_plot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_repetition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: load_weights() missing 1 required positional argument: 'filename'"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    training_mode = False\n",
    "    load_model = False\n",
    "    FILE_NAME = os.path.basename('ppo').split('.')[0]\n",
    "    action_type = 0\n",
    "\n",
    "    env = AvoidReavers(move_angle=10, move_dist=2, frames_per_step=16\n",
    "                       , verbose=0, action_type=action_type, no_gui=False, local_speed=30)\n",
    "\n",
    "    ACTION_SIZE = env.action_space.n\n",
    "\n",
    "    continuous = False if action_type == 0 else True\n",
    "\n",
    "    # Build models\n",
    "    actor = None\n",
    "    ADVANTAGE = Input(shape=(1,))\n",
    "    OLD_PREDICTION = Input(shape=(ACTION_SIZE,))\n",
    "\n",
    "    if continuous:\n",
    "        actor = build_actor_continuous(STATE_SIZE, ACTION_SIZE, ADVANTAGE, OLD_PREDICTION)\n",
    "    else:\n",
    "        actor = build_actor(STATE_SIZE, ACTION_SIZE, ADVANTAGE, OLD_PREDICTION)\n",
    "\n",
    "    critic = build_critic(STATE_SIZE)\n",
    "\n",
    "    agent = PPOAgent(STATE_SIZE, ACTION_SIZE, continuous, actor, critic, GAMMA, LOSS_CLIPPING, EPOCHS, NOISE, ENTROPY_LOSS,\n",
    "                     BUFFER_SIZE,BATCH_SIZE, processor=ReaverProcessor())\n",
    "\n",
    "    agent.compile(optimizer=[Adam(lr=LR), Adam(lr=LR)], metrics=[ADVANTAGE, OLD_PREDICTION])\n",
    "\n",
    "    cb_plot = DrawTrainMovingAvgPlotCallback(os.path.realpath('' + FILE_NAME + '.png'), 10, 5, l_label=['episode_reward'])\n",
    "    agent.load_weights(os.path.realpath('../../../../../'), 'ppo' + '.h5f')\n",
    "    agent.run(env, NB_STEPS, train_mode=training_mode, verbose=2, callbacks=[cb_plot], action_repetition=1, nb_episodes=1000)\n",
    "\n",
    "    if training_mode:\n",
    "        agent.save_weights(os.path.realpath(''), 'ppo' + 'h5f')\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
