{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2019 SAMSUNG SDS <Team.SAIDA@gmail.com>\n",
    "#\n",
    "# This code is distribued under the terms and conditions from the MIT License (MIT).\n",
    "#\n",
    "# Authors : Uk Jo, Iljoo Yoon, Hyunjae Lee, Daehun Jun\n",
    "\n",
    "\n",
    "from core.policies import EpsGreedyQPolicy, GreedyQPolicy, LinearAnnealedPolicy\n",
    "from core.algorithm.DQN import DQNAgent\n",
    "from core.memories import SequentialMemory\n",
    "\n",
    "from keras.layers import Dense, Reshape\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from core.common.processor import Processor\n",
    "from saida_gym.starcraft.avoidReavers import AvoidReavers\n",
    "from core.callbacks import DrawTrainMovingAvgPlotCallback\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_GUI = True\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "EPSILON = 1\n",
    "ENABLE_DOUBLE = True\n",
    "ENABLE_DUELING = False\n",
    "BATCH_SIZE = 256\n",
    "REPLAY_BUFFER_SIZE = 10000\n",
    "LEARNING_RATE = 0.005\n",
    "TARGET_MODEL_UPDATE_INTERVAL = 20\n",
    "WINDOW_LENGTH = 1\n",
    "MAX_STEP_CNT = 50000\n",
    "STATE_SIZE = 8 + 3 * 8\n",
    "\n",
    "\n",
    "def scale_velocity(v):\n",
    "    return v\n",
    "\n",
    "\n",
    "def scale_angle(angle):\n",
    "    return (angle - math.pi) / math.pi\n",
    "\n",
    "\n",
    "def scale_pos(pos):\n",
    "    return pos / 16\n",
    "\n",
    "\n",
    "def scale_pos2(pos):\n",
    "    return pos / 8\n",
    "\n",
    "\n",
    "def exponential_average(old, new, b1):\n",
    "    return old * b1 + (1-b1) * new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReaverProcessor(Processor):\n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "        self.success_cnt = 0\n",
    "        self.cumulate_reward = 0\n",
    "\n",
    "    def process_action(self, action):\n",
    "        self.last_action = action\n",
    "        return action\n",
    "    \n",
    "    # Reshape the reward in a way you want\n",
    "    def reward_reshape(self, reward, state):\n",
    "        \"\"\" Reshape the reward\n",
    "            Starcraft Env returns the reward according to following conditions.\n",
    "            1. Invalid action : -0.1\n",
    "            2. get hit : -1\n",
    "            3. goal : 1\n",
    "            4. others : 0\n",
    "\n",
    "        # Argument\n",
    "            reward (float): The observed reward after executing the action\n",
    "\n",
    "        # Returns\n",
    "            reshaped reward\n",
    "        \"\"\"\n",
    "        \n",
    "        if math.fabs(reward + 0.1) < 0.01:\n",
    "            reward = -1\n",
    "        elif reward == -1:\n",
    "            reward = -10\n",
    "        elif reward == 1:\n",
    "            reward = 10\n",
    "        elif reward == 0:\n",
    "            reward = -1\n",
    "        \n",
    "        # 목표지점과의 거리에 따른 보상 추가\n",
    "        reward += ((state[2] + state[3])/640)*2\n",
    "        return reward\n",
    "    \n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        state_array = self.process_observation(observation)\n",
    "        reward = self.reward_reshape(reward, state_array)\n",
    "        self.cumulate_reward += reward\n",
    "\n",
    "        if reward == 10:\n",
    "            if self.cumulate_reward > 0:\n",
    "                self.success_cnt += 1\n",
    "\n",
    "            self.cumulate_reward = 0\n",
    "            print(\"success_cnt = \", self.success_cnt)\n",
    "\n",
    "        return state_array, reward, done, info\n",
    "\n",
    "    def process_observation(self, observation, **kwargs):\n",
    "        \"\"\" Pre-process observation\n",
    "\n",
    "        # Argument\n",
    "            observation (object): The current observation from the environment.\n",
    "\n",
    "        # Returns\n",
    "            processed observation\n",
    "\n",
    "        \"\"\"\n",
    "        if len(observation.my_unit) > 0:\n",
    "            s = np.zeros(STATE_SIZE)\n",
    "            me = observation.my_unit[0]\n",
    "            # Observation for Dropship\n",
    "            s[0] = scale_pos2(me.pos_x)  # X of coordinates\n",
    "            s[1] = scale_pos2(me.pos_y)  # Y of coordinates\n",
    "            s[2] = scale_pos2(me.pos_x - 320)  # relative X of coordinates from goal\n",
    "            s[3] = scale_pos2(me.pos_y - 320)  # relative Y of coordinates from goal\n",
    "            s[4] = scale_velocity(me.velocity_x)  # X of velocity\n",
    "            s[5] = scale_velocity(me.velocity_y)  # y of coordinates\n",
    "            s[6] = scale_angle(me.angle)  # Angle of head of dropship\n",
    "            s[7] = 1 if me.accelerating else 0  # True if Dropship is accelerating\n",
    "\n",
    "            # Observation for Reavers\n",
    "            for ind, ob in enumerate(observation.en_unit):\n",
    "                s[ind * 8 + 8] = scale_pos2(ob.pos_x - me.pos_x)  # X of relative coordinates\n",
    "                s[ind * 8 + 9] = scale_pos2(ob.pos_y - me.pos_y)  # Y of relative coordinates\n",
    "                s[ind * 8 + 10] = scale_pos2(ob.pos_x - 320)  # X of relative coordinates\n",
    "                s[ind * 8 + 11] = scale_pos2(ob.pos_y - 320)  # Y of relative coordinates\n",
    "                s[ind * 8 + 12] = scale_velocity(ob.velocity_x)  # X of velocity\n",
    "                s[ind * 8 + 13] = scale_velocity(ob.velocity_y)  # Y of velocity\n",
    "                s[ind * 8 + 14] = scale_angle(ob.angle)  # Angle of head of Reavers\n",
    "                s[ind * 8 + 15] = 1 if ob.accelerating else 0  # True if Reaver is accelerating\n",
    "\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    training_mode = True\n",
    "    load_model = False\n",
    "    FILE_NAME = os.path.basename('result').split('.')[0]\n",
    "    action_type = 0\n",
    "    # todo : need to substitute it with env.make() and also remove other parameters such as protobuf_name & verbose!?\n",
    "    env = AvoidReavers(move_angle=15, move_dist=2, frames_per_step=16, verbose=0, action_type=action_type, no_gui=True, local_speed=0)\n",
    "    \n",
    "    try:\n",
    "\n",
    "        state_size = STATE_SIZE\n",
    "        action_size = env.action_space.n\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Reshape((state_size*WINDOW_LENGTH,), input_shape=(WINDOW_LENGTH, state_size)))\n",
    "        model.add(Dense(50, input_dim=state_size*WINDOW_LENGTH, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(50, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(action_size, activation='linear', kernel_initializer='he_uniform'))\n",
    "        model.summary()\n",
    "\n",
    "        memory = SequentialMemory(limit=REPLAY_BUFFER_SIZE, window_length=WINDOW_LENGTH, enable_per=False, per_alpha=0.6)\n",
    "\n",
    "        policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05, nb_steps=20000)\n",
    "        #policy = EpsGreedyQPolicy(EPSILON)\n",
    "        test_policy = GreedyQPolicy()\n",
    "\n",
    "        agent = DQNAgent(model, action_size, memory, processor=ReaverProcessor(), policy=policy, test_policy=test_policy\n",
    "                         , enable_double=ENABLE_DOUBLE, enable_dueling=ENABLE_DUELING, train_interval=80, discount_factor=DISCOUNT_FACTOR\n",
    "                         , batch_size=BATCH_SIZE, target_model_update=TARGET_MODEL_UPDATE_INTERVAL)\n",
    "\n",
    "        agent.compile(Adam(lr=LEARNING_RATE))\n",
    "\n",
    "        callbacks = []\n",
    "\n",
    "        if training_mode:\n",
    "            cb_plot = DrawTrainMovingAvgPlotCallback(\n",
    "                os.path.realpath(FILE_NAME + '.png'), 5, 5, l_label=['episode_reward'])\n",
    "            callbacks.append(cb_plot)\n",
    "            # h5f = 'vulture_vs_zealot_v0_DQN_double_False_dueling_True_batch_size_128_repm_size_200000_lr_0.001_tn_u_invl_100_window_length_2'\n",
    "            # agent.load_weights(os.path.realpath('save_model/' + h5f + '.h5f'))\n",
    "        else:\n",
    "            agent.load_weights(os.path.realpath('../../../../../' + 'result_DoubleDQN_20191103014828' + '.h5f'))\n",
    "\n",
    "        agent.run(env, MAX_STEP_CNT, train_mode=training_mode, verbose=2, callbacks=callbacks)\n",
    "\n",
    "        if training_mode:\n",
    "            agent.save_weights(os.path.realpath(FILE_NAME + '.h5f'), True)\n",
    "\n",
    "    finally:\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
