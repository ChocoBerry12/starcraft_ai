{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent 정의\n",
    "* baseline 을 상속하거나\n",
    "* 직접 구현한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from core.common.agent import Agent\n",
    "from core.common.util import *\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Inherit Agent class as a parent class\n",
    "class DeepSARSAgent(Agent):\n",
    "    \n",
    "    # Detailed description about input parameters see API Doc\n",
    "    def __init__(self, action_size, model, load_model=False, discount_factor=0.99, learning_rate=0.001,\n",
    "             epsilon=1, epsilon_decay=0.999, epsilon_min=0.01,\n",
    "             file_path='', training_mode=True, **kwargs):\n",
    "        \n",
    "        # Call constructor of parent's class\n",
    "        super(DeepSARSAgent, self).__init__(**kwargs)\n",
    "        \n",
    "        # Set parameters from inputs\n",
    "        self.load_model = load_model\n",
    "        self.action_size = action_size\n",
    "        self.model = model\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.training_mode = training_mode\n",
    "        self.file_path = file_path\n",
    "        \n",
    "        # Set the epsilon as minimum value, if not training mode.\n",
    "        if not self.training_mode:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        # memory for train (S,A,R,S',A')\n",
    "        self.observations = deque(maxlen=2)\n",
    "        self.recent_observation = None\n",
    "        self.recent_action = None\n",
    "\n",
    "\n",
    "        if self.load_model and os.path.isfile(file_path):\n",
    "            self.load_weights(file_path)\n",
    "    \n",
    "   # Get an action to be taken from observation\n",
    "    def forward(self, observation):\n",
    "        \n",
    "        # Take a random acton with probability = epsilon\n",
    "        if self.training_mode and np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "        # Take a best acton with probability = (1 - epsilon)\n",
    "            state = np.float32(observation)\n",
    "            q_values = self.model.predict(np.expand_dims(state, 0))\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        # set memory for training\n",
    "        self.recent_observation = observation\n",
    "        self.recent_action = action\n",
    "\n",
    "        return [action]\n",
    "        \n",
    "    # Updates the agent's network\n",
    "    def backward(self, reward, terminal):\n",
    "        \n",
    "        self.observations.append([self.recent_observation, self.recent_action, reward, terminal])\n",
    "\n",
    "        if self.step == 0:\n",
    "            return\n",
    "\n",
    "        # Decaying the epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Use a memory to train\n",
    "        experience = self.observations.popleft()\n",
    "        state = np.float32(experience[0])\n",
    "        action = experience[1]\n",
    "        reward = experience[2]\n",
    "        done = experience[3]\n",
    "\n",
    "        # Get next action on next state from current model\n",
    "        next_state = np.float32(self.recent_observation)\n",
    "        next_action = self.forward(next_state)\n",
    "\n",
    "        # Compute Q values for target network update\n",
    "        # Q(S,A) <- Q(S,A) + alpha(R + gammaQ(S',A') - Q(S,A))\n",
    "        target = self.model.predict(np.expand_dims(state, 0))[0]\n",
    "        if done:\n",
    "            target[action] = reward\n",
    "        else:\n",
    "            target[action] = (reward + self.discount_factor *\n",
    "                              self.model.predict(np.expand_dims(next_state, 0))[0][next_action])\n",
    "\n",
    "        target = np.reshape(target, [1, self.action_size])\n",
    "\n",
    "        self.model.fit(np.expand_dims(state, 0), target, epochs=1, verbose=0)\n",
    "        return\n",
    "\n",
    "    # Compile the model\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "        return\n",
    "    \n",
    "    # Load trained weight from an HDF5 file.\n",
    "    def load_weights(self, filepath) :\n",
    "        self.model.load_weights(filepath)\n",
    "        return\n",
    "\n",
    "    # Save trained weight from an HDF5 file.\n",
    "    def save_weights(self, filepath, overwrite):\n",
    "        self.model.save_weights(filepath, overwrite)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env 생성\n",
    "* env 를 생성한다. \n",
    "* shared memory 방식 혹은 다른 방식으로 게임과 통신한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import saida_gym.starcraft.avoidZerglings as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize...\n",
      "Shared Memory create\u0000\n",
      "SAIDA_ZVZ26880 Shared memory found.\n"
     ]
    }
   ],
   "source": [
    "env = gym.AvoidZerglings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 커스터마이징\n",
    "* state, reward 커스텀\n",
    "* Processor 모듈을 건들면 댐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.common.processor import Processor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class ReaverProcessor(Processor):\n",
    "\n",
    "    # Process an performed action \n",
    "    def process_action(self, action):\n",
    "        \"do what you want with the action\"\n",
    "        return action\n",
    "\n",
    "    # Process the data given from environment after the step finished.\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        state_array = self.process_observation(observation)\n",
    "        reward = self.reward_reshape(reward)\n",
    "        return state_array, reward, done, info\n",
    "\n",
    "    # Reshape your reward (Optional)\n",
    "    def reward_reshape(self, reward):\n",
    "        # Maybe I can give more incentive to the agent, when the agent has reached the goal.\n",
    "        if reward == 1 :  \n",
    "            reward = reward * 2\n",
    "        # And prevent to stay at safe start position rather then moving, give a small negative reward in every step.\n",
    "        elif reward == -1 :\n",
    "            reward  = -0.1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    # Process the raw observation given from environment\n",
    "    def process_observation(self, observation, **kwargs):\n",
    "        # Raw observation data is the form of JSON(precisely, Protobuf).\n",
    "        # Therefore, we need to transform it into an array or something can be calculated.\n",
    "        \n",
    "        # Define the size of state array. \n",
    "        # This time, I need 23 numbers to make the state, which consists of 5 factors of agent observation and 6 from 3 enemies one.\n",
    "        STATE_SIZE = 5 + 3 * 6  \n",
    "        s = np.zeros(STATE_SIZE) # Make an empty array.\n",
    "        me = observation.my_unit[0] # Observation for Dropship (Agent)\n",
    "        # Scale data set in order to learn fast and efficiently.\n",
    "        s[0] = scale_pos(me.pos_x)  # X of coordinates\n",
    "        s[1] = scale_pos(me.pos_y)  # Y of coordinates\n",
    "        s[2] = scale_velocity(me.velocity_x)  # X of velocity\n",
    "        s[3] = scale_velocity(me.velocity_y)  # y of coordinates\n",
    "        s[4] = scale_angle(me.angle)  # Angle of head of dropship\n",
    "\n",
    "        # Observation for Reavers(3 of them)\n",
    "        for ind, ob in enumerate(observation.en_unit):\n",
    "            s[ind * 6 + 5] = scale_pos(ob.pos_x - me.pos_x)  # X of relative coordinates\n",
    "            s[ind * 6 + 6] = scale_pos(ob.pos_y - me.pos_y)  # Y of relative coordinates\n",
    "            s[ind * 6 + 7] = scale_velocity(ob.velocity_x)  # X of velocity\n",
    "            s[ind * 6 + 8] = scale_velocity(ob.velocity_y)  # Y of velocity\n",
    "            s[ind * 6 + 9] = scale_angle(ob.angle)  # Angle of head of Reavers\n",
    "            s[ind * 6 + 10] = scale_angle(1 if ob.accelerating else 0)  # True if Reaver is accelerating\n",
    "\n",
    "        return s\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_velocity(v):\n",
    "        return v\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_angle(angle):\n",
    "        return (angle - math.pi) / math.pi\n",
    "\n",
    "    @staticmethod\n",
    "    def scale_pos(pos):\n",
    "        return int(pos / 16)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그 다음 할 것들..\n",
    "* 신경망 만들고 인자로 넘겨주기\n",
    "* 학습시키기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
