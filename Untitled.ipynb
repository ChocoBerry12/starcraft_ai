{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LSTM version with global and local observation\n",
    "\n",
    "Observation design\n",
    "1. instead of using spatial map #1 with Observers' location in local area within 12 tiles from scurge,\n",
    "   two vectors with lengths 64 having one hot encoding value to represent scurge' absoulte path.\n",
    "   This value will be combined with last action vectors\n",
    "\n",
    "2. spatial map #2 with Scurge's location in entire map\n",
    "\n",
    "3. last action\n",
    "\n",
    "lstm time step 10 based on below\n",
    "\"DRQN is trained using backpropagation through time for the last ten timesteps. Thus both the non-recurrent 10-frame DQN and the recurrent 1-frame DRQN have access to the same history of game screens.3 Thus, when dealing with partial observability, a choice exists between using a non- recurrent deep network with a long history of observations or using a recurrent network trained with a single observa- tion at each timestep. The results in this section show that recurrent networks can integrate information through time and serve as a viable alternative to stacking frames in the input layer of a convoluational network\"\n",
    "\"\"\"\n",
    "\n",
    "from saida_gym.starcraft.avoidObservers import AvoidObservers\n",
    "import saida_gym.envs.conn.connection_env as Config\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense, Input, Concatenate, Conv2D, Flatten, TimeDistributed, LSTM\n",
    "from core.common.processor import Processor\n",
    "from core.callbacks import DrawTrainMovingAvgPlotCallback\n",
    "from core.algorithm.DDPG import DDPGAgent\n",
    "from core.memories import SequentialMemory\n",
    "from core.common.util import *\n",
    "from core.common.random import *\n",
    "from core.policies import *\n",
    "import sys\n",
    "import argparse\n",
    "from core.common.util import OPS\n",
    "# note: add l2 regularizers for critic layers\n",
    "from keras import regularizers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize...\n",
      "Shared Memory create\u0000\n",
      "SAIDA_AO26140 Shared memory found.\n",
      "action space : Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "====================================================================== \n",
    "FOR hyper parameters combination parameter setting from auto launcher\n",
    "======================================================================   \n",
    "\"\"\"\n",
    "'''parser = argparse.ArgumentParser(description='DQN Configuration including setting dqn / double dqn / double dueling dqn')\n",
    "\n",
    "parser.add_argument(OPS.NO_GUI.value, help='gui', type=bool, default=False)\n",
    "parser.add_argument(OPS.USE_PARAMETERIZED_NOISE.value, type=bool, default=False, help=\"Parameterized Action noise\")\n",
    "parser.add_argument(OPS.FRAMES_PER_STEP.value, help='frames', type=int, default=4)\n",
    "parser.add_argument(OPS.BATCH_SIZE.value, type=int, default=128, help=\"batch size\")\n",
    "parser.add_argument(OPS.REPLAY_MEMORY_SIZE.value, type=int, default=8000, help=\"replay memory size\")\n",
    "parser.add_argument(OPS.LEARNING_ACTOR_RATE.value, type=float, default=1e-4, help=\"actor learning rate\")\n",
    "parser.add_argument(OPS.LEARNING_CRITIC_RATE.value, type=float, default=1e-3, help=\"critic learning rate\")\n",
    "parser.add_argument(OPS.TARGET_NETWORK_UPDATE.value, type=int, default=60, help=\"target_network_update_interval\")\n",
    "parser.add_argument(OPS.N_STEPS.value, type=int, default=100000, help=\"n steps for training\")\n",
    "parser.add_argument(OPS.ACTION_REPETITION.value, type=int, default=1, help=\"Action repetition\")\n",
    "parser.add_argument(OPS.OU_SIGMA.value, type=float, default=0.1, help=\"Random noise parameter\")\n",
    "parser.add_argument(OPS.OU_THETA.value, type=float, default=0.15, help=\"Random noise parameter\")\n",
    "parser.add_argument(OPS.TIME_WINDOW.value, type=int, default=2, help=\"Temporal Splice Size\")\n",
    "parser.add_argument(OPS.MARGINAL_SPACE.value, type=int, default=1, help=\"Value for marginal space surrounded by safe zone to give bad panelty if a agent die\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "dict_args = vars(args)\n",
    "post_fix = ''\n",
    "\n",
    "\n",
    "for k in dict_args.keys():\n",
    "    if k == 'no_gui' or  k == 'nsteps' or k == 'critic_lr' or k == 'actor_lr' or k == 'ou_theta':\n",
    "        continue\n",
    "    post_fix += '_' + k + '_' + str(dict_args[k])\n",
    "\n",
    "print('post_fix : {}'.format(post_fix))\n",
    "'''\n",
    "CURRENT_FILE_NAME = os.path.basename('').split('.')[0]\n",
    "CURRENT_FILE_PATH = os.path.sep.join(os.path.abspath('').split(os.path.sep)[:-1])\n",
    "\n",
    "yyyymmdd = mmdd24hhmmss()\n",
    "\n",
    "# FILE_NAME_FOR_LOG = os.path.basename(__file__).split('.')[0] + \"_\" + yyyymmdd\n",
    "FILE_NAME_FOR_LOG = os.path.basename('').split('.')[0] + \"_\" + yyyymmdd + ''\n",
    "\n",
    "\n",
    "# For training\n",
    "TIME_WINDOW = 2\n",
    "BATCH_SIZE = 128\n",
    "MOVING_AVERAGE_WINDOW = 100\n",
    "PLOT_EPISODE_INTERVAL = 200\n",
    "MARGINAL_SPACE = 1  # tile unit\n",
    "\n",
    "STATE1_SIZE = (TIME_WINDOW, 64, 64, 1)  # If you never set it, then it will be \"channels_last\".\n",
    "STATE2_SIZE = (TIME_WINDOW, 20, 20, 1)\n",
    "\n",
    "ACTION_SIZE = 1\n",
    "TRAINING_MODE = True\n",
    "\n",
    "CRITIC_L2_REG = 0.01\n",
    "REWARD_SCALE = 1\n",
    "LOCAL_OBSERVABLE_TILE_SIZE = 10\n",
    "\n",
    "\n",
    "env = AvoidObservers( action_type=1, verbose=0, frames_per_step=4, no_gui=False)\n",
    "print('action space :', env.action_space)\n",
    "observation_input = [Input(shape=STATE1_SIZE, name='scurge_observation_input'), Input(shape=STATE2_SIZE, name='observer_observation_input')]\n",
    "action_input = Input(shape=(ACTION_SIZE, ), name='action_input')\n",
    "\n",
    "\"\"\"\n",
    "inspired by here \n",
    "https://www.reddit.com/r/MachineLearning/comments/8etje4/d_actor_critic_algorithm_why_we_can_share/\n",
    "https://github.com/yanpanlau/DDPG-Keras-Torcs/issues/11\n",
    "\n",
    "here actor and actor network shared convolution layer.  \n",
    "\"\"\"\n",
    "\n",
    "# todo : confirm whether or not we can apply l2 regularization to shared layers between critic and actor network\n",
    "SHARED_CONV2D_1_1 = TimeDistributed(Conv2D(10, kernel_size=5, strides=1, activation='relu', padding='SAME', kernel_regularizer=regularizers.l2(CRITIC_L2_REG)))\n",
    "SHARED_CONV2D_1_2 = TimeDistributed(Conv2D(5, kernel_size=3, strides=1, activation='relu', padding='SAME', kernel_regularizer=regularizers.l2(CRITIC_L2_REG)))\n",
    "SHARED_FLATTEN_1 = TimeDistributed(Flatten())\n",
    "\n",
    "SHARED_CONV2D_2_1 = TimeDistributed(Conv2D(10, kernel_size=4, strides=1, activation='relu', padding='SAME', kernel_regularizer=regularizers.l2(CRITIC_L2_REG)))\n",
    "SHARED_CONV2D_2_2 = TimeDistributed(Conv2D(5, kernel_size=3, strides=1, activation='relu', padding='SAME', kernel_regularizer=regularizers.l2(CRITIC_L2_REG)))\n",
    "SHARED_FLATTEN_2 = TimeDistributed(Flatten())\n",
    "\n",
    "SHARED_CONCATENATED = Concatenate()\n",
    "\n",
    "\n",
    "def build_critic_model():\n",
    "    oh1 = SHARED_CONV2D_1_1(observation_input[0])\n",
    "    oh1 = SHARED_CONV2D_1_2(oh1)\n",
    "    oh1 = SHARED_FLATTEN_1(oh1)\n",
    "\n",
    "    oh2 = SHARED_CONV2D_2_1(observation_input[1])\n",
    "    oh2 = SHARED_CONV2D_2_2(oh2)\n",
    "    oh2 = SHARED_FLATTEN_2(oh2)\n",
    "\n",
    "    oh = SHARED_CONCATENATED([oh1, oh2])\n",
    "    oh = LSTM(512)(oh)\n",
    "\n",
    "    ah = Dense(30, activation='relu', kernel_regularizer=regularizers.l2(CRITIC_L2_REG))(action_input)\n",
    "    ah = Dense(20, activation='relu', kernel_regularizer=regularizers.l2(CRITIC_L2_REG))(ah)\n",
    "\n",
    "    h = Concatenate()([oh, ah])\n",
    "    h = Dense(30, activation='relu', kernel_regularizer=regularizers.l2(CRITIC_L2_REG))(h)\n",
    "    h = Dense(20, activation='relu', kernel_regularizer=regularizers.l2(CRITIC_L2_REG))(h)\n",
    "    output = Dense(1, activation='linear', kernel_regularizer=regularizers.l2(CRITIC_L2_REG))(h)\n",
    "\n",
    "    model = Model(inputs=[observation_input[0], observation_input[1], action_input], outputs=[output])\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_actor_model():\n",
    "    oh1 = SHARED_CONV2D_1_1(observation_input[0])\n",
    "    oh1 = SHARED_CONV2D_1_2(oh1)\n",
    "    oh1 = SHARED_FLATTEN_1(oh1)\n",
    "\n",
    "    oh2 = SHARED_CONV2D_2_1(observation_input[1])\n",
    "    oh2 = SHARED_CONV2D_2_2(oh2)\n",
    "    oh2 = SHARED_FLATTEN_2(oh2)\n",
    "\n",
    "    oh = SHARED_CONCATENATED([oh1, oh2])\n",
    "\n",
    "    h = TimeDistributed(Dense(30, activation='relu'))(oh)\n",
    "    h = LSTM(512)(h)\n",
    "    h = Dense(20, activation='relu')(h)\n",
    "\n",
    "    output = Dense(ACTION_SIZE, activation='sigmoid')(h)\n",
    "    model = Model(inputs=observation_input, outputs=[output])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObsProcessor(Processor):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ObsProcessor, self).__init__(**kwargs)\n",
    "\n",
    "        # self.accumulated_observation = deque(maxlen=3)\n",
    "        # self.state_size = state_size\n",
    "        self.highest_height = 1900\n",
    "        # self.deque_last_actions = deque(maxlen=TIME_WINDOW)\n",
    "        self.last_action = None\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        state = self.process_observation(observation)\n",
    "        reward = self.reward_shape(observation, done)\n",
    "        return state, reward, done, info\n",
    "\n",
    "    def reward_shape(self, observation, done):\n",
    "        \"\"\"\n",
    "        reward range\n",
    "        :param observation:\n",
    "        :param done:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Goal 에 도달하거나 죽으면\n",
    "        if done:\n",
    "            self.highest_height = 1900\n",
    "            if 0 < observation.my_unit[0].pos_y and observation.my_unit[0].pos_y < 65 + MARGINAL_SPACE:\n",
    "                return 10 * REWARD_SCALE\n",
    "            # Safe zone : left-top (896, 1888) right-bottom (1056, 2048) with additional (marginal) space -> more penalty\n",
    "            elif 896 - 32*MARGINAL_SPACE >= observation.my_unit[0].pos_x and observation.my_unit[0].pos_x <= 1056 + 32*MARGINAL_SPACE and observation.my_unit[0].pos_y >= 1888 - 32*MARGINAL_SPACE:\n",
    "                return -10 * REWARD_SCALE\n",
    "            return -5 * REWARD_SCALE\n",
    "\n",
    "        # give important weight per height rank\n",
    "        # 0 ~ 1888(59 tiles) / 32 : ratio\n",
    "        if observation.my_unit[0].pos_y < self.highest_height:\n",
    "            rank = int(observation.my_unit[0].pos_y / 32)  # 2 ~ 59\n",
    "            weight = (59 / (rank + sys.float_info.epsilon)) / 59\n",
    "            self.highest_height = observation.my_unit[0].pos_y\n",
    "            return weight * 3 * REWARD_SCALE\n",
    "\n",
    "        # 시간이 지나면\n",
    "        return -0.02 * REWARD_SCALE\n",
    "\n",
    "\n",
    "    def process_observation(self, observation, **kwargs):\n",
    "        # making spatial map with size 10 x 10 and its channels include velocity_x, velocity_y, angle, accelerating_yn\n",
    "        # we change this shape ignoring velocity_x, velocity_y, angle, accelerating_yn. Instead,\n",
    "        # we give another spatial map which includes flags for walkable(out of map), safe zone\n",
    "        map_of_scurge = np.zeros(shape=(64, 64))  # this map is for scurge (64, 64), global observation\n",
    "        map_of_observer = np.zeros(shape=(LOCAL_OBSERVABLE_TILE_SIZE*2, LOCAL_OBSERVABLE_TILE_SIZE*2))  # observer map, local observation centered by sculge's position\n",
    "        # map_of_map = np.zeros(shape=(64, 64))  # this map is\n",
    "\n",
    "        me_x = observation.my_unit[0].pos_x\n",
    "        me_y = observation.my_unit[0].pos_y\n",
    "\n",
    "        me_x_t = np.clip(int(observation.my_unit[0].pos_x/32), 0, 64)\n",
    "        me_y_t = np.clip(int(observation.my_unit[0].pos_y/32), 0, 64)\n",
    "\n",
    "        # Safe zone : left-top (0, 0) right-bottom (64*32, 2048) with additional (marginal) space\n",
    "        for x in range(int(896/32), int(1056/32)):\n",
    "            for y in range(int(1888/32), int(2048/32)):\n",
    "                # map_of_scurge[x][y] = -1  # masking safe zone\n",
    "                map_of_scurge[y][x] = -1  # masking safe zone\n",
    "\n",
    "        # Safe zone : left-top (896, 1888) right-bottom (1056, 2048) with additional (marginal) space\n",
    "        for x in range(int(0/32), int(2048/32)):\n",
    "            for y in range(int(0/32), int(64/32)):\n",
    "                # map_of_scurge[x][y] = -1  # masking safe zone\n",
    "                map_of_scurge[y][x] = -1  # masking safe zone\n",
    "\n",
    "        # map_of_scurge[me_x_t][me_y_t] = 1\n",
    "        map_of_scurge[me_y_t][me_x_t] = 1\n",
    "\n",
    "        map_of_scurge = np.expand_dims(map_of_scurge, -1)\n",
    "\n",
    "        for ob in observation.en_unit:\n",
    "            # if 1880 <= ob.pos_y and 880 <= ob.pos_x <= 1050:\n",
    "            #     pass\n",
    "            # else:\n",
    "            en_x_t = ob.pos_x / 32\n",
    "            en_y_t = ob.pos_y / 32\n",
    "\n",
    "            rel_x = int(en_x_t - me_x_t) + LOCAL_OBSERVABLE_TILE_SIZE\n",
    "            rel_y = int(en_y_t - me_y_t) + LOCAL_OBSERVABLE_TILE_SIZE\n",
    "\n",
    "            rel_x = np.clip(rel_x, 0, LOCAL_OBSERVABLE_TILE_SIZE*2-1)\n",
    "            rel_y = np.clip(rel_y, 0, LOCAL_OBSERVABLE_TILE_SIZE*2-1)\n",
    "\n",
    "            # map_of_observer[rel_x][rel_y] = map_of_observer[rel_x][rel_y] + 1  # if two or more observers are duplicated, we use sum\n",
    "            map_of_observer[rel_y][rel_x] = map_of_observer[rel_y][rel_x] + 1  # if two or more observers are duplicated, we use sum\n",
    "\n",
    "        # display out of map where scurge can't go based on current location of scurge\n",
    "        scurge_out_of_map_left = me_x_t - LOCAL_OBSERVABLE_TILE_SIZE\n",
    "        scurge_out_of_map_right = me_x_t + LOCAL_OBSERVABLE_TILE_SIZE\n",
    "        scurge_out_of_map_up = me_y_t - LOCAL_OBSERVABLE_TILE_SIZE\n",
    "        scurge_out_of_map_down = me_y_t + LOCAL_OBSERVABLE_TILE_SIZE\n",
    "\n",
    "        if scurge_out_of_map_left < 0:\n",
    "            map_of_observer[:, 0:-scurge_out_of_map_left] = -1\n",
    "        if scurge_out_of_map_right > 64:\n",
    "            map_of_observer[:, -(scurge_out_of_map_right-64):] = -1\n",
    "        if scurge_out_of_map_up < 0:\n",
    "            map_of_observer[0:-scurge_out_of_map_up,:] = -1\n",
    "        if scurge_out_of_map_down > 64:\n",
    "            map_of_observer[-(scurge_out_of_map_down-64):,:] = -1\n",
    "\n",
    "        map_of_observer = np.expand_dims(map_of_observer, -1)\n",
    "\n",
    "        if self.last_action is None:\n",
    "            self.last_action = [-1]\n",
    "\n",
    "        return [map_of_scurge, map_of_observer, self.last_action]\n",
    "\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "            \"\"\"\n",
    "            BATCH_SIZE + TIME_WINDOW + OBSERVATION SPACE1 #1 (12,12)\n",
    "            + OBSERVATION SPACE #2 (64,64) + OBSERVATION SPACE #3 (ACTIONS DURING TIME_WINDOWS,)\n",
    "\n",
    "            :param batch:\n",
    "            :return:\n",
    "            \"\"\"\n",
    "\n",
    "            # here, skip assert in case that there could be another case that batch size = 1 for forward..\n",
    "            # assert batch.shape[0] == BATCH_SIZE\n",
    "            assert batch.shape[1] == TIME_WINDOW\n",
    "\n",
    "            # observation needs to be concatenated all previous frame into one stacked frame.\n",
    "            # but if time window is 1, it just squeeze data instead.\n",
    "            # batch = np.squeeze(batch, axis=1)\n",
    "\n",
    "            observation_1 = []\n",
    "            observation_2 = []\n",
    "            # observation_3 = []   # batch\n",
    "\n",
    "            for i, sample in enumerate(batch):\n",
    "                observation_sample_1 = []\n",
    "                observation_sample_2 = []\n",
    "                observation_sample_3 = []   # time window\n",
    "                for t in range(TIME_WINDOW):\n",
    "                    observation_sample_1.append(batch[i][t][0])  # sculge observation\n",
    "                    observation_sample_2.append(batch[i][t][1])  # observers observation\n",
    "                    # observation_sample_3.append([-1] if batch[i][t][2] is None else batch[i][t][2])  # last actions\n",
    "                    # observation_sample_3.append( [0]*ACTION_SIZE if batch[i][t][2] is None else np.eye(ACTION_SIZE)[int(batch[i][t][2][0])] )  # last actions\n",
    "\n",
    "                observation_1.append(observation_sample_1)\n",
    "                observation_2.append(observation_sample_2)\n",
    "                # observation_3.append(observation_sample_3)\n",
    "\n",
    "            observation_1 = np.asarray(observation_1)\n",
    "            observation_2 = np.asarray(observation_2)\n",
    "            # observation_3 = np.asarray(observation_3)\n",
    "\n",
    "            # if len(observation_3.shape) == 2:\n",
    "            #     assert observation_3.shape == (TIME_WINDOW, 1)\n",
    "            #     observation_3 = np.expand_dims(observation_3, axis=0)\n",
    "\n",
    "            # if TIME_WINDOW == 1:\n",
    "            #     observation_1 = np.squeeze(observation_1, axis=1)\n",
    "            #     observation_2 = np.squeeze(observation_2, axis=1)\n",
    "            #     observation_3 = np.squeeze(observation_3, axis=1)\n",
    "\n",
    "            # return [observation_1, observation_2, observation_3]\n",
    "\n",
    "            return [observation_1, observation_2]\n",
    "\n",
    "\n",
    "    def process_action(self, action):\n",
    "        act = []\n",
    "        actions = []\n",
    "        act.append(4)  # radiuqs tile position)\n",
    "        act.append(action)  # angle between 0 and 1\n",
    "        act.append(0)   # move(0) attack(1)\n",
    "        act[1] = np.clip(act[1], 0, 1)\n",
    "        actions.append(act)\n",
    "\n",
    "        self.last_action = act[1]\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "scurge_observation_input (Input (None, 2, 64, 64, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observer_observation_input (Inp (None, 2, 20, 20, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 2, 64, 64, 10 260         scurge_observation_input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 2, 20, 20, 10 170         observer_observation_input[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 2, 64, 64, 5) 455         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 2, 20, 20, 5) 455         time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 2, 20480)     0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 2, 2000)      0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 22480)     0           time_distributed_3[0][0]         \n",
      "                                                                 time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 2, 30)        674430      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 512)          1112064     time_distributed_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 20)           10260       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            21          dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,798,115\n",
      "Trainable params: 1,798,115\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "scurge_observation_input (Input (None, 2, 64, 64, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "observer_observation_input (Inp (None, 2, 20, 20, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 2, 64, 64, 10 260         scurge_observation_input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 2, 20, 20, 10 170         observer_observation_input[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 2, 64, 64, 5) 455         time_distributed_1[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 2, 20, 20, 5) 455         time_distributed_4[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 2, 20480)     0           time_distributed_2[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 2, 2000)      0           time_distributed_5[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "action_input (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2, 22480)     0           time_distributed_3[1][0]         \n",
      "                                                                 time_distributed_6[1][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 30)           60          action_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 512)          47089664    concatenate_1[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           620         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 532)          0           lstm_2[0][0]                     \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 30)           15990       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           620         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            21          dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 47,108,315\n",
      "Trainable params: 47,108,315\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Starting training for 100000 steps ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-ee44a78dea6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mcb_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDrawTrainMovingAvgPlotCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../save_graph/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mFILE_NAME_FOR_LOG\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPLOT_EPISODE_INTERVAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMOVING_AVERAGE_WINDOW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'episode_reward'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRAINING_MODE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcb_plot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_repetition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../save_model/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mCURRENT_FILE_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCURRENT_FILE_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myyyymmdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\SAIDA_RL\\python\\core\\common\\agent.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, env, nb_steps, shared_cb_params, train_mode, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, random_policy, log_interval, nb_max_episode_steps, nb_episodes)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[1;31m# Obtain the initial observation by resetting the environment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                 \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\SAIDA_RL\\python\\saida_gym\\envs\\SAIDAGym.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMESSAGE_TYPE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Reset\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_reset_msg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_req_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"Reset\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\SAIDA_RL\\python\\saida_gym\\envs\\conn\\shm.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, end_on_close)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m             \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[0msleepTime\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;31m# if sleepTime > 30 :\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "memory = SequentialMemory(limit=1000, window_length=TIME_WINDOW)\n",
    "actor = build_actor_model()\n",
    "critic = build_critic_model()\n",
    "\n",
    "policy = NoisePolicy(\n",
    "    random_process=SimpleOUNoise(size=1, theta=.15, mu=0., sigma=.1))\n",
    "test_policy = NoisePolicy(\n",
    "        random_process=SimpleOUNoise(size=1, theta=.15, mu=0., sigma=.1))\n",
    "\n",
    "# 0.1 for actor, 0.5 for critic\n",
    "agent = DDPGAgent(actor, critic, ACTION_SIZE, memory, critic_action_input=action_input,\n",
    "train_interval=60,\n",
    "                  processor=ObsProcessor(), batch_size=BATCH_SIZE, tau_for_actor=1e-3, tau_for_critic=1e-2, policy=policy, test_policy=test_policy)\n",
    "\n",
    "# [optimizer for critic, optimizer for actor]\n",
    "agent.compile(optimizer=[\n",
    "    Adam(lr=1e-3, clipvalue=0.5),\n",
    "    Adam(lr=1e-4, clipvalue=0.5)]\n",
    ")\n",
    "\n",
    "\n",
    "cb_plot = DrawTrainMovingAvgPlotCallback('../../save_graph/' + FILE_NAME_FOR_LOG + '.png', PLOT_EPISODE_INTERVAL, MOVING_AVERAGE_WINDOW, l_label=['episode_reward'])\n",
    "\n",
    "agent.run(env, 100000, train_mode=TRAINING_MODE, verbose=2, callbacks=[cb_plot], action_repetition=1)\n",
    "\n",
    "agent.save_weights('../../save_model/' + CURRENT_FILE_PATH, CURRENT_FILE_NAME, yyyymmdd, True)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
