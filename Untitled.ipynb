{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.common.agent import Agent\n",
    "from core.common.util import *\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Inherit Agent class as a parent class\n",
    "class DeepSARSAgent(Agent):\n",
    "    \n",
    "    # Detailed description about input parameters see API Doc\n",
    "    def __init__(self, action_size, model, load_model=False, discount_factor=0.99, learning_rate=0.001,\n",
    "                 epsilon=1, epsilon_decay=0.999, epsilon_min=0.01,\n",
    "                 file_path='', training_mode=True, **kwargs):\n",
    "        \n",
    "        # Call constructor of parent's class\n",
    "        super(DeepSARSAgent, self).__init__(**kwargs)\n",
    "\n",
    "        # Set parameters from inputs\n",
    "        self.load_model = load_model\n",
    "        self.action_size = action_size\n",
    "        self.model = model\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.training_mode = training_mode\n",
    "        self.file_path = file_path\n",
    "\n",
    "        # Set the epsilon as minimum value, if not training mode.\n",
    "        if not self.training_mode:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        # memory for train (S,A,R,S',A')\n",
    "        self.observations = deque(maxlen=2)\n",
    "        self.recent_observation = None\n",
    "        self.recent_action = None\n",
    "\n",
    "\n",
    "        if self.load_model and os.path.isfile(file_path):\n",
    "            self.load_weights(file_path)\n",
    "    \n",
    "    # Get an action to be taken from observation\n",
    "    def forward(self, observation):\n",
    "        \n",
    "        # Take a random acton with probability = epsilon\n",
    "        if self.training_mode and np.random.rand() <= self.epsilon:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "        # Take a best acton with probability = (1 - epsilon)\n",
    "            state = np.float32(observation)\n",
    "            q_values = self.model.predict(np.expand_dims(state, 0))\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        # set memory for training\n",
    "        self.recent_observation = observation\n",
    "        self.recent_action = action\n",
    "\n",
    "        return [action]\n",
    "        \n",
    "    # Updates the agent's network\n",
    "    def backward(self, reward, terminal):\n",
    "        \n",
    "        self.observations.append([self.recent_observation, self.recent_action, reward, terminal])\n",
    "\n",
    "        if self.step == 0:\n",
    "            return\n",
    "\n",
    "        # Decaying the epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        # Use a memory to train\n",
    "        experience = self.observations.popleft()\n",
    "        state = np.float32(experience[0])\n",
    "        action = experience[1]\n",
    "        reward = experience[2]\n",
    "        done = experience[3]\n",
    "\n",
    "        # Get next action on next state from current model\n",
    "        next_state = np.float32(self.recent_observation)\n",
    "        next_action = self.forward(next_state)\n",
    "\n",
    "        # Compute Q values for target network update\n",
    "        # Q(S,A) <- Q(S,A) + alpha(R + gammaQ(S',A') - Q(S,A))\n",
    "        target = self.model.predict(np.expand_dims(state, 0))[0]\n",
    "        if done:\n",
    "            target[action] = reward\n",
    "        else:\n",
    "            target[action] = (reward + self.discount_factor *\n",
    "                              self.model.predict(np.expand_dims(next_state, 0))[0][next_action])\n",
    "\n",
    "        target = np.reshape(target, [1, self.action_size])\n",
    "\n",
    "        self.model.fit(np.expand_dims(state, 0), target, epochs=1, verbose=0)\n",
    "        return\n",
    "\n",
    "    # Compile the model\n",
    "    def compile(self, optimizer, metrics=[]):\n",
    "        self.model.compile(optimizer=optimizer, loss='mse')\n",
    "        return\n",
    "    \n",
    "    # Load trained weight from an HDF5 file.\n",
    "    def load_weights(self, filepath) :\n",
    "        self.model.load_weights(filepath)\n",
    "        return\n",
    "\n",
    "    # Save trained weight from an HDF5 file.\n",
    "    def save_weights(self, filepath, overwrite):\n",
    "        self.model.save_weights(filepath, overwrite)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 지정된 파일을 찾을 수 없습니다: '..\\\\..\\\\..\\\\cpp\\\\Release\\\\SAIDA\\\\SAIDA.exe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-62bab169e08e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msaida_gym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstarcraft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavoidReavers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAvoidReavers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAvoidReavers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove_angle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmove_dist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframes_per_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvsersion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\SAIDA_RL\\python\\saida_gym\\starcraft\\avoidReavers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, shm_name, **kwargs)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mAvoidReavers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSAIDAGym\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshm_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAIDA_AR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'AvoidReavers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshm_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmake_initial_msg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\SAIDA_RL\\python\\saida_gym\\envs\\SAIDAGym.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, gym_name, version, action_type, frames_per_step, move_angle, move_dist, verbose, protobuf_name, connect_type, bot_runner, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m                  , bot_runner=r\"..\\..\\..\\cpp\\Release\\SAIDA\\SAIDA.exe\", **kwargs):\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbot_runner\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbot_runner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Initialize...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다: '..\\\\..\\\\..\\\\cpp\\\\Release\\\\SAIDA\\\\SAIDA.exe'"
     ]
    }
   ],
   "source": [
    "from saida_gym.starcraft.avoidReavers import AvoidReavers\n",
    "env = AvoidReavers(action_type=0, move_angle=30, move_dist=2, frames_per_step=24, vsersion=0, verbose=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
