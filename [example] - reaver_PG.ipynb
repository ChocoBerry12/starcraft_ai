{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Copyright (C) 2019 SAMSUNG SDS <Team.SAIDA@gmail.com>\n",
    "#\n",
    "# This code is distribued under the terms and conditions from the MIT License (MIT).\n",
    "#\n",
    "# Authors : Uk Jo, Iljoo Yoon, Hyunjae Lee, Daehun Jun\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime\n",
    "from core.common.processor import Processor\n",
    "from core.algorithm.REINFORCE import ReinforceAgent\n",
    "from core.callbacks import DrawTrainMovingAvgPlotCallback\n",
    "from saida_gym.starcraft.avoidReavers import AvoidReavers\n",
    "import saida_gym.envs.conn.connection_env as Config\n",
    "\n",
    "from keras.layers import Dense, Reshape\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameter\n",
    "EPISODES = 1000\n",
    "LEARNING_RATE = 0.0004\n",
    "# 5 features of a Dropship  + 6 features of 3 Reavers\n",
    "STATE_SIZE = 5 + 3 * 6\n",
    "\n",
    "\n",
    "def scale_velocity(v):\n",
    "    return v\n",
    "\n",
    "\n",
    "def scale_coordinate(pos):\n",
    "    return np.tanh(int(pos / 16) / 20)\n",
    "\n",
    "\n",
    "def scale_angle(angle):\n",
    "    return (angle - math.pi) / math.pi\n",
    "\n",
    "\n",
    "def scale_pos(pos):\n",
    "    return int(pos / 16)\n",
    "\n",
    "\n",
    "def scale_pos2(pos):\n",
    "    return int(pos / 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReaverProcessor(Processor):\n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "\n",
    "    def process_action(self, action):\n",
    "        self.last_action = action\n",
    "        return action\n",
    "    \n",
    "    def reward_reshape(self, reward, state):\n",
    "        if math.fabs(reward + 0.1) < 0.01:\n",
    "            reward = -5\n",
    "        elif reward == 0:\n",
    "            reward = -0.1\n",
    "        elif reward == -1:\n",
    "            reward = -100\n",
    "        elif reward == 1:\n",
    "            reward = 2\n",
    "\n",
    "        reward += ((state[2] + state[3])/640)*.5\n",
    "        return reward\n",
    "    \n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        state_array = self.process_observation(observation)\n",
    "        reward = self.reward_reshape(reward, state_array)\n",
    "        return state_array, reward, done, info\n",
    "\n",
    "    def process_observation(self, observation, **kwargs):\n",
    "        \"\"\" Pre-process observation\n",
    "\n",
    "        # Argument\n",
    "            observation (object): The current observation from the environment.\n",
    "\n",
    "        # Returns\n",
    "            processed observation\n",
    "\n",
    "        \"\"\"\n",
    "        if len(observation.my_unit) > 0:\n",
    "            s = np.zeros(STATE_SIZE)\n",
    "            me = observation.my_unit[0]\n",
    "            # Observation for Dropship\n",
    "            s[0] = scale_pos(me.pos_x)  # X of coordinates\n",
    "            s[1] = scale_pos(me.pos_y)  # Y of coordinates\n",
    "            s[2] = scale_velocity(me.velocity_x)  # X of velocity\n",
    "            s[3] = scale_velocity(me.velocity_y)  # y of coordinates\n",
    "            s[4] = scale_angle(me.angle)  # Angle of head of dropship\n",
    "\n",
    "            # Observation for Reavers\n",
    "            for ind, ob in enumerate(observation.en_unit):\n",
    "                s[ind * 6 + 5] = scale_pos(ob.pos_x - me.pos_x)  # X of relative coordinates\n",
    "                s[ind * 6 + 6] = scale_pos(ob.pos_y - me.pos_y)  # Y of relative coordinates\n",
    "                s[ind * 6 + 7] = scale_velocity(ob.velocity_x)  # X of velocity\n",
    "                s[ind * 6 + 8] = scale_velocity(ob.velocity_y)  # Y of velocity\n",
    "                s[ind * 6 + 9] = scale_angle(ob.angle)  # Angle of head of Reavers\n",
    "                s[ind * 6 + 10] = scale_angle(1 if ob.accelerating else 0)  # True if Reaver is accelerating\n",
    "\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize...\n",
      "Shared Memory create\u0000\n",
      "SAIDA_AR15580 Shared memory found.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 70)                1680      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 50)                3550      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 40)                2040      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 13)                403       \n",
      "=================================================================\n",
      "Total params: 8,903\n",
      "Trainable params: 8,903\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Starting training for 30000 steps ...\n",
      "    95/30000: episode: 1, duration: 3.616s, episode steps: 95, steps per second: 26, episode reward: -957.385, mean reward: -10.078 [-100.003, 100.000], mean action: 6.453 [0.000, 12.000]\n",
      "   805/30000: episode: 2, duration: 20.536s, episode steps: 710, steps per second: 35, episode reward: -13214.855, mean reward: -18.612 [-100.003, 100.000], mean action: 6.697 [0.000, 12.000]\n",
      "  1471/30000: episode: 3, duration: 22.519s, episode steps: 666, steps per second: 30, episode reward: -12117.260, mean reward: -18.194 [-100.003, 100.000], mean action: 6.698 [0.000, 12.000]\n",
      "  1506/30000: episode: 4, duration: 1.312s, episode steps: 35, steps per second: 27, episode reward: -222.687, mean reward: -6.362 [-100.001, 100.000], mean action: 5.343 [0.000, 12.000]\n",
      "  1790/30000: episode: 5, duration: 11.294s, episode steps: 284, steps per second: 25, episode reward: -5338.669, mean reward: -18.798 [-100.003, 100.000], mean action: 6.567 [0.000, 12.000]\n",
      "  1933/30000: episode: 6, duration: 6.696s, episode steps: 143, steps per second: 21, episode reward: -2624.286, mean reward: -18.352 [-100.003, 100.000], mean action: 6.734 [0.000, 12.000]\n",
      "  2096/30000: episode: 7, duration: 8.523s, episode steps: 163, steps per second: 19, episode reward: -2952.386, mean reward: -18.113 [-100.003, 100.000], mean action: 6.890 [0.000, 12.000]\n",
      "  2249/30000: episode: 8, duration: 6.542s, episode steps: 153, steps per second: 23, episode reward: -586.181, mean reward: -3.831 [-100.002, 100.000], mean action: 6.595 [0.000, 12.000]\n",
      "  2265/30000: episode: 9, duration: 0.761s, episode steps: 16, steps per second: 21, episode reward: -125.787, mean reward: -7.862 [-100.000, 100.000], mean action: 5.312 [1.000, 12.000]\n",
      "  2453/30000: episode: 10, duration: 9.958s, episode steps: 188, steps per second: 19, episode reward: -2833.485, mean reward: -15.072 [-100.003, 100.000], mean action: 6.011 [0.000, 12.000]\n",
      "  2860/30000: episode: 11, duration: 25.898s, episode steps: 407, steps per second: 16, episode reward: -6253.064, mean reward: -15.364 [-100.003, 100.000], mean action: 6.887 [0.000, 12.000]\n",
      "  3061/30000: episode: 12, duration: 10.650s, episode steps: 201, steps per second: 19, episode reward: -3458.684, mean reward: -17.207 [-100.003, 100.000], mean action: 6.318 [0.000, 12.000]\n",
      "  3184/30000: episode: 13, duration: 7.646s, episode steps: 123, steps per second: 16, episode reward: -2897.487, mean reward: -23.557 [-100.003, 100.000], mean action: 6.504 [0.000, 12.000]\n",
      "  3514/30000: episode: 14, duration: 17.793s, episode steps: 330, steps per second: 19, episode reward: -5970.158, mean reward: -18.091 [-100.003, 100.000], mean action: 6.697 [0.000, 12.000]\n",
      "  3535/30000: episode: 15, duration: 1.043s, episode steps: 21, steps per second: 20, episode reward: -116.483, mean reward: -5.547 [-100.000, 100.000], mean action: 5.952 [0.000, 12.000]\n",
      "  3829/30000: episode: 16, duration: 14.154s, episode steps: 294, steps per second: 21, episode reward: -5275.970, mean reward: -17.945 [-100.003, 100.000], mean action: 6.680 [0.000, 12.000]\n",
      "  3875/30000: episode: 17, duration: 2.069s, episode steps: 46, steps per second: 22, episode reward: -233.583, mean reward: -5.078 [-100.000, 100.000], mean action: 5.826 [0.000, 12.000]\n",
      "  4032/30000: episode: 18, duration: 6.894s, episode steps: 157, steps per second: 23, episode reward: -2815.681, mean reward: -17.934 [-100.003, 100.000], mean action: 6.637 [0.000, 12.000]\n",
      "  4166/30000: episode: 19, duration: 6.037s, episode steps: 134, steps per second: 22, episode reward: -2328.580, mean reward: -17.377 [-100.003, 100.000], mean action: 6.657 [0.000, 12.000]\n",
      "  4285/30000: episode: 20, duration: 5.299s, episode steps: 119, steps per second: 22, episode reward: -2412.278, mean reward: -20.271 [-100.003, 100.000], mean action: 6.529 [0.000, 12.000]\n",
      "  4355/30000: episode: 21, duration: 4.034s, episode steps: 70, steps per second: 17, episode reward: -769.785, mean reward: -10.997 [-100.002, 100.000], mean action: 6.657 [0.000, 12.000]\n",
      "  4442/30000: episode: 22, duration: 3.871s, episode steps: 87, steps per second: 22, episode reward: -1126.984, mean reward: -12.954 [-100.003, 100.000], mean action: 6.230 [0.000, 12.000]\n",
      "  4546/30000: episode: 23, duration: 4.619s, episode steps: 104, steps per second: 23, episode reward: -1236.484, mean reward: -11.889 [-100.003, 100.000], mean action: 6.673 [0.000, 12.000]\n",
      "  4588/30000: episode: 24, duration: 1.938s, episode steps: 42, steps per second: 22, episode reward: -437.882, mean reward: -10.426 [-99.999, 100.000], mean action: 5.810 [0.000, 12.000]\n",
      "  4672/30000: episode: 25, duration: 3.857s, episode steps: 84, steps per second: 22, episode reward: -1195.280, mean reward: -14.230 [-100.003, 100.000], mean action: 6.024 [1.000, 12.000]\n",
      "  4732/30000: episode: 26, duration: 3.406s, episode steps: 60, steps per second: 18, episode reward: -1273.184, mean reward: -21.220 [-100.001, 100.000], mean action: 6.433 [0.000, 12.000]\n",
      "  5133/30000: episode: 27, duration: 17.682s, episode steps: 401, steps per second: 23, episode reward: -7426.765, mean reward: -18.521 [-100.003, 100.000], mean action: 6.155 [0.000, 12.000]\n",
      "  5391/30000: episode: 28, duration: 10.416s, episode steps: 258, steps per second: 25, episode reward: -4091.269, mean reward: -15.858 [-100.003, 100.000], mean action: 6.570 [0.000, 12.000]\n",
      "  5421/30000: episode: 29, duration: 1.297s, episode steps: 30, steps per second: 23, episode reward: 53.016, mean reward: 1.767 [-5.001, 100.000], mean action: 5.233 [0.000, 11.000]\n",
      "  5564/30000: episode: 30, duration: 5.960s, episode steps: 143, steps per second: 24, episode reward: -2370.576, mean reward: -16.577 [-100.003, 100.000], mean action: 6.063 [0.000, 12.000]\n",
      "  5729/30000: episode: 31, duration: 7.041s, episode steps: 165, steps per second: 23, episode reward: -3934.983, mean reward: -23.848 [-100.003, 100.000], mean action: 6.515 [0.000, 12.000]\n",
      "  5930/30000: episode: 32, duration: 8.180s, episode steps: 201, steps per second: 25, episode reward: -2808.378, mean reward: -13.972 [-100.002, 100.000], mean action: 6.692 [0.000, 12.000]\n",
      "  6078/30000: episode: 33, duration: 6.081s, episode steps: 148, steps per second: 24, episode reward: -2853.983, mean reward: -19.284 [-100.002, 100.000], mean action: 6.601 [0.000, 12.000]\n",
      "  6164/30000: episode: 34, duration: 3.752s, episode steps: 86, steps per second: 23, episode reward: -1429.579, mean reward: -16.623 [-100.002, 100.000], mean action: 6.698 [1.000, 12.000]\n",
      "  6188/30000: episode: 35, duration: 1.071s, episode steps: 24, steps per second: 22, episode reward: -231.389, mean reward: -9.641 [-100.003, 100.000], mean action: 6.792 [1.000, 12.000]\n",
      "  6293/30000: episode: 36, duration: 4.697s, episode steps: 105, steps per second: 22, episode reward: -1896.683, mean reward: -18.064 [-100.002, 100.000], mean action: 6.743 [0.000, 12.000]\n",
      "  6374/30000: episode: 37, duration: 3.549s, episode steps: 81, steps per second: 23, episode reward: -780.679, mean reward: -9.638 [-100.002, 100.000], mean action: 6.519 [0.000, 12.000]\n",
      "  6391/30000: episode: 38, duration: 0.734s, episode steps: 17, steps per second: 23, episode reward: 83.712, mean reward: 4.924 [-5.000, 100.000], mean action: 5.941 [1.000, 12.000]\n",
      "  6518/30000: episode: 39, duration: 5.610s, episode steps: 127, steps per second: 23, episode reward: -2164.278, mean reward: -17.042 [-100.002, 100.000], mean action: 6.693 [0.000, 12.000]\n",
      "  6639/30000: episode: 40, duration: 5.147s, episode steps: 121, steps per second: 24, episode reward: -1898.275, mean reward: -15.688 [-100.003, 100.000], mean action: 5.992 [0.000, 12.000]\n",
      "  6942/30000: episode: 41, duration: 13.786s, episode steps: 303, steps per second: 22, episode reward: -5959.567, mean reward: -19.669 [-100.003, 100.000], mean action: 6.205 [0.000, 12.000]\n",
      "  6979/30000: episode: 42, duration: 1.662s, episode steps: 37, steps per second: 22, episode reward: -527.481, mean reward: -14.256 [-100.003, 100.000], mean action: 5.676 [0.000, 12.000]\n",
      "  7045/30000: episode: 43, duration: 2.863s, episode steps: 66, steps per second: 23, episode reward: -1563.683, mean reward: -23.692 [-100.002, 100.000], mean action: 6.318 [0.000, 12.000]\n",
      "  7195/30000: episode: 44, duration: 6.290s, episode steps: 150, steps per second: 24, episode reward: -1616.185, mean reward: -10.775 [-100.003, 100.000], mean action: 6.813 [0.000, 12.000]\n",
      "  7381/30000: episode: 45, duration: 8.205s, episode steps: 186, steps per second: 23, episode reward: -1537.579, mean reward: -8.267 [-100.002, 100.000], mean action: 6.618 [0.000, 12.000]\n",
      "  7393/30000: episode: 46, duration: 1.058s, episode steps: 12, steps per second: 11, episode reward: -100.886, mean reward: -8.407 [-100.000, 100.000], mean action: 3.750 [0.000, 11.000]\n",
      "  7448/30000: episode: 47, duration: 2.241s, episode steps: 55, steps per second: 25, episode reward: -458.783, mean reward: -8.342 [-100.001, 100.000], mean action: 6.418 [0.000, 12.000]\n",
      "  7523/30000: episode: 48, duration: 3.299s, episode steps: 75, steps per second: 23, episode reward: -974.978, mean reward: -13.000 [-100.001, 100.000], mean action: 5.867 [0.000, 12.000]\n",
      "  7610/30000: episode: 49, duration: 3.745s, episode steps: 87, steps per second: 23, episode reward: -1141.688, mean reward: -13.123 [-100.003, 100.000], mean action: 6.402 [0.000, 12.000]\n",
      "  7741/30000: episode: 50, duration: 5.363s, episode steps: 131, steps per second: 24, episode reward: -2572.183, mean reward: -19.635 [-100.003, 100.000], mean action: 6.313 [0.000, 12.000]\n",
      "  7770/30000: episode: 51, duration: 1.603s, episode steps: 29, steps per second: 18, episode reward: -416.988, mean reward: -14.379 [-100.000, 100.000], mean action: 5.552 [0.000, 12.000]\n",
      "  7878/30000: episode: 52, duration: 4.515s, episode steps: 108, steps per second: 24, episode reward: -1711.878, mean reward: -15.851 [-100.003, 100.000], mean action: 6.333 [0.000, 12.000]\n",
      "  8063/30000: episode: 53, duration: 7.768s, episode steps: 185, steps per second: 24, episode reward: -2153.478, mean reward: -11.640 [-100.003, 100.000], mean action: 6.573 [0.000, 12.000]\n",
      "  8094/30000: episode: 54, duration: 1.451s, episode steps: 31, steps per second: 21, episode reward: -346.688, mean reward: -11.183 [-100.003, 100.000], mean action: 5.710 [0.000, 12.000]\n",
      "  8149/30000: episode: 55, duration: 2.369s, episode steps: 55, steps per second: 23, episode reward: -388.285, mean reward: -7.060 [-100.002, 100.000], mean action: 6.982 [0.000, 12.000]\n",
      "  8218/30000: episode: 56, duration: 3.222s, episode steps: 69, steps per second: 21, episode reward: -448.484, mean reward: -6.500 [-100.003, 100.000], mean action: 6.942 [0.000, 12.000]\n",
      "  8306/30000: episode: 57, duration: 3.631s, episode steps: 88, steps per second: 24, episode reward: -1675.580, mean reward: -19.041 [-100.003, 100.000], mean action: 6.534 [0.000, 12.000]\n",
      "  8418/30000: episode: 58, duration: 4.454s, episode steps: 112, steps per second: 25, episode reward: -2162.786, mean reward: -19.311 [-100.003, 100.000], mean action: 6.214 [0.000, 12.000]\n",
      "  8672/30000: episode: 59, duration: 9.943s, episode steps: 254, steps per second: 26, episode reward: -4940.972, mean reward: -19.453 [-100.003, 100.000], mean action: 6.504 [0.000, 12.000]\n",
      "  8728/30000: episode: 60, duration: 2.230s, episode steps: 56, steps per second: 25, episode reward: -688.085, mean reward: -12.287 [-100.001, 100.000], mean action: 6.482 [0.000, 12.000]\n",
      "  8763/30000: episode: 61, duration: 1.827s, episode steps: 35, steps per second: 19, episode reward: -247.187, mean reward: -7.062 [-99.999, 100.000], mean action: 6.143 [0.000, 12.000]\n",
      "  8924/30000: episode: 62, duration: 6.578s, episode steps: 161, steps per second: 24, episode reward: -1734.882, mean reward: -10.776 [-100.002, 100.000], mean action: 7.050 [0.000, 12.000]\n",
      "  9231/30000: episode: 63, duration: 13.167s, episode steps: 307, steps per second: 23, episode reward: -4905.159, mean reward: -15.978 [-100.003, 100.000], mean action: 6.827 [0.000, 12.000]\n",
      "  9556/30000: episode: 64, duration: 14.481s, episode steps: 325, steps per second: 22, episode reward: -3881.569, mean reward: -11.943 [-100.002, 100.000], mean action: 6.652 [0.000, 12.000]\n",
      "  9592/30000: episode: 65, duration: 1.689s, episode steps: 36, steps per second: 21, episode reward: -537.189, mean reward: -14.922 [-100.001, 100.000], mean action: 5.750 [1.000, 12.000]\n",
      "  9823/30000: episode: 66, duration: 10.005s, episode steps: 231, steps per second: 23, episode reward: -2921.072, mean reward: -12.645 [-100.003, 100.000], mean action: 6.450 [0.000, 12.000]\n",
      "  9879/30000: episode: 67, duration: 2.475s, episode steps: 56, steps per second: 23, episode reward: -393.282, mean reward: -7.023 [-100.001, 100.000], mean action: 7.107 [0.000, 12.000]\n",
      "  9907/30000: episode: 68, duration: 1.230s, episode steps: 28, steps per second: 23, episode reward: -441.388, mean reward: -15.764 [-100.002, 100.000], mean action: 5.464 [0.000, 12.000]\n",
      " 10022/30000: episode: 69, duration: 5.245s, episode steps: 115, steps per second: 22, episode reward: -3106.282, mean reward: -27.011 [-100.003, 100.000], mean action: 5.783 [0.000, 12.000]\n",
      " 10121/30000: episode: 70, duration: 4.051s, episode steps: 99, steps per second: 24, episode reward: -1671.778, mean reward: -16.887 [-100.002, 100.000], mean action: 6.293 [1.000, 12.000]\n",
      " 10207/30000: episode: 71, duration: 4.354s, episode steps: 86, steps per second: 20, episode reward: -976.081, mean reward: -11.350 [-100.002, 100.000], mean action: 6.535 [0.000, 12.000]\n",
      " 10295/30000: episode: 72, duration: 3.915s, episode steps: 88, steps per second: 22, episode reward: -900.885, mean reward: -10.237 [-100.001, 100.000], mean action: 6.705 [0.000, 12.000]\n",
      " 10559/30000: episode: 73, duration: 11.039s, episode steps: 264, steps per second: 24, episode reward: -5344.559, mean reward: -20.245 [-100.003, 100.000], mean action: 6.595 [0.000, 12.000]\n",
      " 11203/30000: episode: 74, duration: 25.585s, episode steps: 644, steps per second: 25, episode reward: -10936.952, mean reward: -16.983 [-100.003, 100.000], mean action: 7.006 [0.000, 12.000]\n",
      " 11366/30000: episode: 75, duration: 6.479s, episode steps: 163, steps per second: 25, episode reward: -2555.781, mean reward: -15.680 [-100.002, 100.000], mean action: 7.135 [0.000, 12.000]\n",
      " 11421/30000: episode: 76, duration: 3.096s, episode steps: 55, steps per second: 18, episode reward: -473.481, mean reward: -8.609 [-100.000, 100.000], mean action: 6.145 [1.000, 12.000]\n",
      " 11487/30000: episode: 77, duration: 2.803s, episode steps: 66, steps per second: 24, episode reward: -1329.579, mean reward: -20.145 [-100.002, 100.000], mean action: 6.485 [0.000, 12.000]\n",
      " 11606/30000: episode: 78, duration: 4.666s, episode steps: 119, steps per second: 26, episode reward: -1922.593, mean reward: -16.156 [-100.003, 100.000], mean action: 5.487 [0.000, 12.000]\n",
      " 11694/30000: episode: 79, duration: 3.524s, episode steps: 88, steps per second: 25, episode reward: -1965.483, mean reward: -22.335 [-100.003, 100.000], mean action: 6.352 [0.000, 12.000]\n",
      " 11742/30000: episode: 80, duration: 2.055s, episode steps: 48, steps per second: 23, episode reward: -1317.988, mean reward: -27.458 [-100.003, 100.000], mean action: 6.146 [0.000, 12.000]\n",
      " 11987/30000: episode: 81, duration: 11.553s, episode steps: 245, steps per second: 21, episode reward: -3578.765, mean reward: -14.607 [-100.003, 100.000], mean action: 6.237 [0.000, 12.000]\n",
      " 12017/30000: episode: 82, duration: 1.434s, episode steps: 30, steps per second: 21, episode reward: -107.589, mean reward: -3.586 [-100.000, 100.000], mean action: 6.400 [1.000, 12.000]\n",
      " 12079/30000: episode: 83, duration: 2.926s, episode steps: 62, steps per second: 21, episode reward: -934.487, mean reward: -15.072 [-100.002, 100.000], mean action: 5.984 [0.000, 12.000]\n",
      " 12228/30000: episode: 84, duration: 6.421s, episode steps: 149, steps per second: 23, episode reward: -1073.582, mean reward: -7.205 [-100.001, 100.000], mean action: 7.027 [1.000, 12.000]\n",
      " 12456/30000: episode: 85, duration: 9.765s, episode steps: 228, steps per second: 23, episode reward: -1761.166, mean reward: -7.724 [-100.003, 100.000], mean action: 6.544 [0.000, 12.000]\n",
      " 12513/30000: episode: 86, duration: 2.942s, episode steps: 57, steps per second: 19, episode reward: -368.880, mean reward: -6.472 [-100.001, 100.000], mean action: 5.930 [0.000, 12.000]\n",
      " 12849/30000: episode: 87, duration: 14.966s, episode steps: 336, steps per second: 22, episode reward: -4879.774, mean reward: -14.523 [-100.003, 100.000], mean action: 6.610 [0.000, 12.000]\n",
      " 12927/30000: episode: 88, duration: 3.526s, episode steps: 78, steps per second: 22, episode reward: -1460.080, mean reward: -18.719 [-100.001, 100.000], mean action: 6.885 [1.000, 12.000]\n",
      " 12957/30000: episode: 89, duration: 1.204s, episode steps: 30, steps per second: 25, episode reward: -356.385, mean reward: -11.879 [-100.000, 100.000], mean action: 6.367 [1.000, 12.000]\n",
      " 13065/30000: episode: 90, duration: 4.801s, episode steps: 108, steps per second: 22, episode reward: -1787.281, mean reward: -16.549 [-100.002, 100.000], mean action: 6.648 [0.000, 12.000]\n",
      " 13081/30000: episode: 91, duration: 1.220s, episode steps: 16, steps per second: 13, episode reward: -230.588, mean reward: -14.412 [-99.998, 100.000], mean action: 3.875 [0.000, 12.000]\n",
      " 13130/30000: episode: 92, duration: 2.069s, episode steps: 49, steps per second: 24, episode reward: -58.581, mean reward: -1.196 [-100.002, 100.000], mean action: 5.735 [0.000, 12.000]\n",
      " 13167/30000: episode: 93, duration: 1.538s, episode steps: 37, steps per second: 24, episode reward: -642.084, mean reward: -17.354 [-100.000, 100.000], mean action: 6.189 [1.000, 12.000]\n",
      " 13523/30000: episode: 94, duration: 14.055s, episode steps: 356, steps per second: 25, episode reward: -3728.969, mean reward: -10.475 [-100.003, 100.000], mean action: 6.652 [0.000, 12.000]\n",
      " 13605/30000: episode: 95, duration: 3.151s, episode steps: 82, steps per second: 26, episode reward: -1599.585, mean reward: -19.507 [-100.002, 100.000], mean action: 6.329 [1.000, 12.000]\n",
      " 13835/30000: episode: 96, duration: 9.647s, episode steps: 230, steps per second: 24, episode reward: -4309.774, mean reward: -18.738 [-100.003, 100.000], mean action: 6.561 [0.000, 12.000]\n",
      " 13887/30000: episode: 97, duration: 1.985s, episode steps: 52, steps per second: 26, episode reward: -368.382, mean reward: -7.084 [-100.002, 100.000], mean action: 6.269 [1.000, 12.000]\n",
      " 13918/30000: episode: 98, duration: 1.310s, episode steps: 31, steps per second: 24, episode reward: -32.288, mean reward: -1.042 [-99.998, 100.000], mean action: 5.677 [0.000, 12.000]\n",
      " 14034/30000: episode: 99, duration: 4.579s, episode steps: 116, steps per second: 25, episode reward: -1788.076, mean reward: -15.414 [-100.003, 100.000], mean action: 6.379 [0.000, 12.000]\n",
      " 14300/30000: episode: 100, duration: 10.926s, episode steps: 266, steps per second: 24, episode reward: -4442.668, mean reward: -16.702 [-100.003, 100.000], mean action: 6.372 [0.000, 12.000]\n",
      " 14355/30000: episode: 101, duration: 3.078s, episode steps: 55, steps per second: 18, episode reward: -378.483, mean reward: -6.882 [-100.001, 100.000], mean action: 6.727 [0.000, 12.000]\n",
      " 14472/30000: episode: 102, duration: 4.462s, episode steps: 117, steps per second: 26, episode reward: -804.977, mean reward: -6.880 [-100.001, 100.000], mean action: 7.111 [1.000, 12.000]\n",
      " 14636/30000: episode: 103, duration: 6.868s, episode steps: 164, steps per second: 24, episode reward: -2129.870, mean reward: -12.987 [-100.003, 100.000], mean action: 6.213 [0.000, 12.000]\n",
      " 14806/30000: episode: 104, duration: 7.239s, episode steps: 170, steps per second: 23, episode reward: -2123.680, mean reward: -12.492 [-100.003, 100.000], mean action: 6.565 [0.000, 12.000]\n",
      " 14879/30000: episode: 105, duration: 3.230s, episode steps: 73, steps per second: 23, episode reward: -548.786, mean reward: -7.518 [-100.001, 100.000], mean action: 6.274 [0.000, 12.000]\n",
      " 15011/30000: episode: 106, duration: 5.961s, episode steps: 132, steps per second: 22, episode reward: -1809.272, mean reward: -13.707 [-100.003, 100.000], mean action: 6.606 [0.000, 12.000]\n",
      " 15076/30000: episode: 107, duration: 2.791s, episode steps: 65, steps per second: 23, episode reward: -1663.481, mean reward: -25.592 [-100.002, 100.000], mean action: 5.708 [0.000, 12.000]\n",
      " 15140/30000: episode: 108, duration: 2.562s, episode steps: 64, steps per second: 25, episode reward: -778.983, mean reward: -12.172 [-100.001, 100.000], mean action: 6.312 [0.000, 12.000]\n",
      " 15169/30000: episode: 109, duration: 1.191s, episode steps: 29, steps per second: 24, episode reward: -346.487, mean reward: -11.948 [-100.001, 100.000], mean action: 5.862 [2.000, 12.000]\n",
      " 15229/30000: episode: 110, duration: 2.429s, episode steps: 60, steps per second: 25, episode reward: -659.084, mean reward: -10.985 [-100.001, 100.000], mean action: 6.367 [1.000, 12.000]\n",
      " 15249/30000: episode: 111, duration: 1.283s, episode steps: 20, steps per second: 16, episode reward: -211.384, mean reward: -10.569 [-99.999, 100.000], mean action: 4.700 [0.000, 12.000]\n",
      " 15450/30000: episode: 112, duration: 8.393s, episode steps: 201, steps per second: 24, episode reward: -2121.879, mean reward: -10.557 [-100.002, 100.000], mean action: 6.692 [0.000, 12.000]\n",
      " 15659/30000: episode: 113, duration: 9.959s, episode steps: 209, steps per second: 21, episode reward: -1888.580, mean reward: -9.036 [-100.003, 100.000], mean action: 6.627 [0.000, 12.000]\n",
      " 15886/30000: episode: 114, duration: 9.722s, episode steps: 227, steps per second: 23, episode reward: -3069.574, mean reward: -13.522 [-100.003, 100.000], mean action: 6.396 [0.000, 12.000]\n",
      " 16034/30000: episode: 115, duration: 6.694s, episode steps: 148, steps per second: 22, episode reward: -1864.778, mean reward: -12.600 [-100.002, 100.000], mean action: 6.595 [0.000, 12.000]\n",
      " 16071/30000: episode: 116, duration: 2.295s, episode steps: 37, steps per second: 16, episode reward: -62.286, mean reward: -1.683 [-99.999, 100.000], mean action: 6.270 [0.000, 12.000]\n",
      " 16204/30000: episode: 117, duration: 5.914s, episode steps: 133, steps per second: 22, episode reward: -2058.177, mean reward: -15.475 [-100.002, 100.000], mean action: 5.925 [0.000, 12.000]\n",
      " 16292/30000: episode: 118, duration: 4.408s, episode steps: 88, steps per second: 20, episode reward: -684.480, mean reward: -7.778 [-100.001, 100.000], mean action: 6.102 [0.000, 12.000]\n",
      " 16481/30000: episode: 119, duration: 7.722s, episode steps: 189, steps per second: 24, episode reward: -3540.780, mean reward: -18.734 [-100.003, 100.000], mean action: 6.561 [0.000, 12.000]\n",
      " 16538/30000: episode: 120, duration: 2.504s, episode steps: 57, steps per second: 23, episode reward: -264.083, mean reward: -4.633 [-100.002, 100.000], mean action: 6.105 [0.000, 12.000]\n",
      " 16582/30000: episode: 121, duration: 2.574s, episode steps: 44, steps per second: 17, episode reward: -632.982, mean reward: -14.386 [-100.001, 100.000], mean action: 5.409 [0.000, 12.000]\n",
      " 16659/30000: episode: 122, duration: 3.275s, episode steps: 77, steps per second: 24, episode reward: -1223.990, mean reward: -15.896 [-100.003, 100.000], mean action: 5.896 [0.000, 12.000]\n",
      " 16778/30000: episode: 123, duration: 4.885s, episode steps: 119, steps per second: 24, episode reward: -1247.781, mean reward: -10.486 [-100.002, 100.000], mean action: 6.739 [0.000, 12.000]\n",
      " 16797/30000: episode: 124, duration: 0.837s, episode steps: 19, steps per second: 23, episode reward: -111.385, mean reward: -5.862 [-99.999, 100.000], mean action: 5.737 [1.000, 12.000]\n",
      " 16906/30000: episode: 125, duration: 4.466s, episode steps: 109, steps per second: 24, episode reward: -1612.077, mean reward: -14.790 [-100.003, 100.000], mean action: 6.413 [0.000, 12.000]\n",
      " 17027/30000: episode: 126, duration: 5.286s, episode steps: 121, steps per second: 23, episode reward: -2312.578, mean reward: -19.112 [-100.003, 100.000], mean action: 6.190 [0.000, 12.000]\n",
      " 17086/30000: episode: 127, duration: 2.409s, episode steps: 59, steps per second: 24, episode reward: -1029.187, mean reward: -17.444 [-100.001, 100.000], mean action: 6.678 [0.000, 12.000]\n",
      " 17176/30000: episode: 128, duration: 3.648s, episode steps: 90, steps per second: 25, episode reward: -1700.276, mean reward: -18.892 [-100.001, 100.000], mean action: 6.333 [0.000, 12.000]\n",
      " 17235/30000: episode: 129, duration: 2.504s, episode steps: 59, steps per second: 24, episode reward: -1268.187, mean reward: -21.495 [-100.003, 100.000], mean action: 5.712 [1.000, 12.000]\n",
      " 17355/30000: episode: 130, duration: 5.448s, episode steps: 120, steps per second: 22, episode reward: -1657.277, mean reward: -13.811 [-100.002, 100.000], mean action: 6.625 [0.000, 12.000]\n",
      " 17402/30000: episode: 131, duration: 2.686s, episode steps: 47, steps per second: 17, episode reward: -143.583, mean reward: -3.055 [-99.999, 100.000], mean action: 6.000 [1.000, 12.000]\n",
      " 17570/30000: episode: 132, duration: 7.371s, episode steps: 168, steps per second: 23, episode reward: -2819.777, mean reward: -16.784 [-100.003, 100.000], mean action: 6.708 [0.000, 12.000]\n",
      " 17626/30000: episode: 133, duration: 2.549s, episode steps: 56, steps per second: 22, episode reward: -848.681, mean reward: -15.155 [-100.002, 100.000], mean action: 5.893 [0.000, 12.000]\n",
      " 17708/30000: episode: 134, duration: 3.691s, episode steps: 82, steps per second: 22, episode reward: -834.684, mean reward: -10.179 [-100.003, 100.000], mean action: 6.329 [0.000, 12.000]\n",
      " 17956/30000: episode: 135, duration: 11.615s, episode steps: 248, steps per second: 21, episode reward: -1932.770, mean reward: -7.793 [-100.001, 100.000], mean action: 7.069 [0.000, 12.000]\n",
      " 18048/30000: episode: 136, duration: 4.496s, episode steps: 92, steps per second: 20, episode reward: -925.783, mean reward: -10.063 [-100.002, 100.000], mean action: 7.174 [0.000, 12.000]\n",
      " 18324/30000: episode: 137, duration: 11.979s, episode steps: 276, steps per second: 23, episode reward: -2376.267, mean reward: -8.610 [-100.002, 100.000], mean action: 7.203 [0.000, 12.000]\n",
      " 18388/30000: episode: 138, duration: 3.182s, episode steps: 64, steps per second: 20, episode reward: -308.883, mean reward: -4.826 [-100.002, 100.000], mean action: 6.750 [1.000, 12.000]\n",
      " 18417/30000: episode: 139, duration: 1.297s, episode steps: 29, steps per second: 22, episode reward: -431.683, mean reward: -14.886 [-100.003, 100.000], mean action: 6.069 [1.000, 12.000]\n",
      " 18441/30000: episode: 140, duration: 1.145s, episode steps: 24, steps per second: 21, episode reward: -316.585, mean reward: -13.191 [-100.002, 100.000], mean action: 5.125 [0.000, 12.000]\n",
      " 18473/30000: episode: 141, duration: 2.028s, episode steps: 32, steps per second: 16, episode reward: -251.785, mean reward: -7.868 [-100.001, 100.000], mean action: 5.875 [1.000, 12.000]\n",
      " 18649/30000: episode: 142, duration: 8.814s, episode steps: 176, steps per second: 20, episode reward: -1500.377, mean reward: -8.525 [-100.001, 100.000], mean action: 6.881 [0.000, 12.000]\n",
      " 18749/30000: episode: 143, duration: 4.254s, episode steps: 100, steps per second: 24, episode reward: -985.383, mean reward: -9.854 [-100.002, 100.000], mean action: 6.790 [1.000, 12.000]\n",
      " 18801/30000: episode: 144, duration: 2.424s, episode steps: 52, steps per second: 21, episode reward: 21.415, mean reward: 0.412 [-5.000, 100.000], mean action: 7.365 [1.000, 12.000]\n",
      " 18862/30000: episode: 145, duration: 2.605s, episode steps: 61, steps per second: 23, episode reward: -349.689, mean reward: -5.733 [-99.999, 100.000], mean action: 6.328 [1.000, 12.000]\n",
      " 19003/30000: episode: 146, duration: 6.698s, episode steps: 141, steps per second: 21, episode reward: -1864.076, mean reward: -13.220 [-100.002, 100.000], mean action: 7.106 [1.000, 12.000]\n",
      " 19071/30000: episode: 147, duration: 2.975s, episode steps: 68, steps per second: 23, episode reward: -798.986, mean reward: -11.750 [-100.001, 100.000], mean action: 7.044 [0.000, 12.000]\n",
      " 19103/30000: episode: 148, duration: 1.383s, episode steps: 32, steps per second: 23, episode reward: -726.786, mean reward: -22.712 [-100.002, 100.000], mean action: 5.469 [0.000, 12.000]\n",
      " 19134/30000: episode: 149, duration: 1.190s, episode steps: 31, steps per second: 26, episode reward: -37.187, mean reward: -1.200 [-99.997, 100.000], mean action: 7.226 [1.000, 12.000]\n",
      " 19178/30000: episode: 150, duration: 1.937s, episode steps: 44, steps per second: 23, episode reward: -557.586, mean reward: -12.672 [-100.003, 100.000], mean action: 5.545 [0.000, 12.000]\n",
      " 19208/30000: episode: 151, duration: 1.922s, episode steps: 30, steps per second: 16, episode reward: -236.884, mean reward: -7.896 [-100.002, 100.000], mean action: 6.500 [0.000, 12.000]\n",
      " 19330/30000: episode: 152, duration: 3.873s, episode steps: 122, steps per second: 32, episode reward: -1567.374, mean reward: -12.847 [-100.001, 100.000], mean action: 7.180 [1.000, 12.000]\n",
      " 19371/30000: episode: 153, duration: 1.286s, episode steps: 41, steps per second: 32, episode reward: -562.181, mean reward: -13.712 [-100.000, 100.000], mean action: 5.732 [1.000, 12.000]\n",
      " 19400/30000: episode: 154, duration: 1.208s, episode steps: 29, steps per second: 24, episode reward: -251.486, mean reward: -8.672 [-100.002, 100.000], mean action: 6.310 [1.000, 12.000]\n",
      " 19511/30000: episode: 155, duration: 4.650s, episode steps: 111, steps per second: 24, episode reward: -1956.080, mean reward: -17.622 [-100.002, 100.000], mean action: 6.595 [1.000, 12.000]\n",
      " 19581/30000: episode: 156, duration: 3.776s, episode steps: 70, steps per second: 19, episode reward: -463.283, mean reward: -6.618 [-100.000, 100.000], mean action: 7.643 [1.000, 12.000]\n",
      " 19643/30000: episode: 157, duration: 2.286s, episode steps: 62, steps per second: 27, episode reward: -1468.280, mean reward: -23.682 [-100.003, 100.000], mean action: 6.323 [1.000, 12.000]\n",
      " 19693/30000: episode: 158, duration: 1.460s, episode steps: 50, steps per second: 34, episode reward: -843.186, mean reward: -16.864 [-100.002, 100.000], mean action: 6.540 [0.000, 12.000]\n",
      " 19713/30000: episode: 159, duration: 0.665s, episode steps: 20, steps per second: 30, episode reward: -316.185, mean reward: -15.809 [-100.001, 100.000], mean action: 5.550 [1.000, 12.000]\n",
      " 19846/30000: episode: 160, duration: 5.116s, episode steps: 133, steps per second: 26, episode reward: -2394.081, mean reward: -18.001 [-100.003, 100.000], mean action: 6.549 [0.000, 12.000]\n",
      " 19904/30000: episode: 161, duration: 2.097s, episode steps: 58, steps per second: 28, episode reward: -318.087, mean reward: -5.484 [-100.000, 100.000], mean action: 7.345 [0.000, 12.000]\n",
      " 19986/30000: episode: 162, duration: 2.769s, episode steps: 82, steps per second: 30, episode reward: -715.185, mean reward: -8.722 [-100.002, 100.000], mean action: 6.854 [1.000, 12.000]\n",
      " 20029/30000: episode: 163, duration: 1.519s, episode steps: 43, steps per second: 28, episode reward: -391.985, mean reward: -9.116 [-100.001, 100.000], mean action: 7.279 [1.000, 12.000]\n",
      " 20042/30000: episode: 164, duration: 0.540s, episode steps: 13, steps per second: 24, episode reward: -200.886, mean reward: -15.453 [-99.998, 100.000], mean action: 4.692 [1.000, 12.000]\n",
      " 20090/30000: episode: 165, duration: 1.645s, episode steps: 48, steps per second: 29, episode reward: -282.785, mean reward: -5.891 [-100.000, 100.000], mean action: 6.708 [1.000, 12.000]\n",
      " 20214/30000: episode: 166, duration: 4.722s, episode steps: 124, steps per second: 26, episode reward: -1121.978, mean reward: -9.048 [-100.002, 100.000], mean action: 6.726 [0.000, 12.000]\n",
      " 20243/30000: episode: 167, duration: 1.052s, episode steps: 29, steps per second: 28, episode reward: -122.184, mean reward: -4.213 [-100.003, 100.000], mean action: 6.448 [1.000, 12.000]\n",
      " 20294/30000: episode: 168, duration: 1.888s, episode steps: 51, steps per second: 27, episode reward: -232.181, mean reward: -4.553 [-100.000, 100.000], mean action: 7.118 [1.000, 12.000]\n",
      " 20475/30000: episode: 169, duration: 6.175s, episode steps: 181, steps per second: 29, episode reward: -2206.971, mean reward: -12.193 [-100.002, 100.000], mean action: 7.094 [0.000, 12.000]\n",
      " 20667/30000: episode: 170, duration: 6.337s, episode steps: 192, steps per second: 30, episode reward: -1850.674, mean reward: -9.639 [-100.003, 100.000], mean action: 7.422 [1.000, 12.000]\n",
      " 20745/30000: episode: 171, duration: 3.576s, episode steps: 78, steps per second: 22, episode reward: -940.984, mean reward: -12.064 [-100.002, 100.000], mean action: 6.141 [0.000, 12.000]\n",
      " 20765/30000: episode: 172, duration: 0.940s, episode steps: 20, steps per second: 21, episode reward: -221.186, mean reward: -11.059 [-100.003, 100.000], mean action: 4.750 [1.000, 12.000]\n",
      " 20924/30000: episode: 173, duration: 4.725s, episode steps: 159, steps per second: 34, episode reward: -2504.479, mean reward: -15.751 [-100.003, 100.000], mean action: 6.610 [0.000, 12.000]\n",
      " 21015/30000: episode: 174, duration: 2.623s, episode steps: 91, steps per second: 35, episode reward: -670.081, mean reward: -7.364 [-100.002, 100.000], mean action: 7.044 [1.000, 12.000]\n",
      " 21161/30000: episode: 175, duration: 4.788s, episode steps: 146, steps per second: 30, episode reward: -1738.279, mean reward: -11.906 [-100.002, 100.000], mean action: 7.233 [0.000, 12.000]\n",
      " 21414/30000: episode: 176, duration: 10.860s, episode steps: 253, steps per second: 23, episode reward: -3119.279, mean reward: -12.329 [-100.003, 100.000], mean action: 7.292 [0.000, 12.000]\n",
      " 21505/30000: episode: 177, duration: 3.244s, episode steps: 91, steps per second: 28, episode reward: -766.979, mean reward: -8.428 [-100.003, 100.000], mean action: 6.374 [0.000, 12.000]\n",
      " 21580/30000: episode: 178, duration: 2.428s, episode steps: 75, steps per second: 31, episode reward: -419.682, mean reward: -5.596 [-100.002, 100.000], mean action: 6.840 [1.000, 12.000]\n",
      " 21689/30000: episode: 179, duration: 3.959s, episode steps: 109, steps per second: 28, episode reward: -620.985, mean reward: -5.697 [-100.001, 100.000], mean action: 7.367 [1.000, 12.000]\n",
      " 21858/30000: episode: 180, duration: 6.323s, episode steps: 169, steps per second: 27, episode reward: -1440.877, mean reward: -8.526 [-100.001, 100.000], mean action: 7.598 [0.000, 12.000]\n",
      " 22132/30000: episode: 181, duration: 10.619s, episode steps: 274, steps per second: 26, episode reward: -2794.178, mean reward: -10.198 [-100.003, 100.000], mean action: 7.252 [0.000, 12.000]\n",
      " 22179/30000: episode: 182, duration: 1.814s, episode steps: 47, steps per second: 26, episode reward: -348.287, mean reward: -7.410 [-100.002, 100.000], mean action: 6.894 [1.000, 12.000]\n",
      " 22323/30000: episode: 183, duration: 4.963s, episode steps: 144, steps per second: 29, episode reward: -2098.486, mean reward: -14.573 [-100.001, 100.000], mean action: 7.438 [0.000, 12.000]\n",
      " 22486/30000: episode: 184, duration: 5.875s, episode steps: 163, steps per second: 28, episode reward: -1359.985, mean reward: -8.343 [-100.003, 100.000], mean action: 7.049 [0.000, 12.000]\n",
      " 22588/30000: episode: 185, duration: 4.464s, episode steps: 102, steps per second: 23, episode reward: -356.777, mean reward: -3.498 [-100.000, 100.000], mean action: 6.461 [1.000, 12.000]\n",
      " 22639/30000: episode: 186, duration: 2.599s, episode steps: 51, steps per second: 20, episode reward: -192.984, mean reward: -3.784 [-99.999, 100.000], mean action: 7.216 [1.000, 12.000]\n",
      " 22721/30000: episode: 187, duration: 3.206s, episode steps: 82, steps per second: 26, episode reward: -834.683, mean reward: -10.179 [-100.003, 100.000], mean action: 6.927 [1.000, 12.000]\n",
      " 22917/30000: episode: 188, duration: 7.611s, episode steps: 196, steps per second: 26, episode reward: -837.375, mean reward: -4.272 [-100.002, 100.000], mean action: 7.505 [0.000, 12.000]\n",
      " 23089/30000: episode: 189, duration: 6.702s, episode steps: 172, steps per second: 26, episode reward: -1614.577, mean reward: -9.387 [-100.003, 100.000], mean action: 7.244 [0.000, 12.000]\n",
      " 23159/30000: episode: 190, duration: 2.872s, episode steps: 70, steps per second: 24, episode reward: -379.980, mean reward: -5.428 [-100.000, 100.000], mean action: 7.500 [0.000, 12.000]\n",
      " 23201/30000: episode: 191, duration: 5.059s, episode steps: 42, steps per second: 8, episode reward: -187.186, mean reward: -4.457 [-99.999, 100.000], mean action: 6.000 [1.000, 12.000]\n",
      " 23311/30000: episode: 192, duration: 3.829s, episode steps: 110, steps per second: 29, episode reward: -1417.285, mean reward: -12.884 [-100.001, 100.000], mean action: 7.245 [1.000, 12.000]\n",
      " 23383/30000: episode: 193, duration: 2.475s, episode steps: 72, steps per second: 29, episode reward: -204.883, mean reward: -2.846 [-100.000, 100.000], mean action: 7.278 [1.000, 12.000]\n",
      " 23407/30000: episode: 194, duration: 0.839s, episode steps: 24, steps per second: 29, episode reward: -155.987, mean reward: -6.499 [-99.998, 100.000], mean action: 6.875 [1.000, 12.000]\n",
      " 23497/30000: episode: 195, duration: 3.357s, episode steps: 90, steps per second: 27, episode reward: -1390.785, mean reward: -15.453 [-100.003, 100.000], mean action: 7.044 [0.000, 12.000]\n",
      " 23711/30000: episode: 196, duration: 7.966s, episode steps: 214, steps per second: 27, episode reward: -2267.173, mean reward: -10.594 [-100.003, 100.000], mean action: 7.425 [0.000, 12.000]\n",
      " 23770/30000: episode: 197, duration: 1.877s, episode steps: 59, steps per second: 31, episode reward: -198.686, mean reward: -3.368 [-100.000, 100.000], mean action: 7.712 [1.000, 12.000]\n",
      " 23856/30000: episode: 198, duration: 2.765s, episode steps: 86, steps per second: 31, episode reward: -1175.882, mean reward: -13.673 [-100.003, 100.000], mean action: 7.372 [0.000, 12.000]\n",
      " 24007/30000: episode: 199, duration: 4.793s, episode steps: 151, steps per second: 32, episode reward: -1443.984, mean reward: -9.563 [-100.001, 100.000], mean action: 7.212 [1.000, 12.000]\n",
      " 24187/30000: episode: 200, duration: 5.208s, episode steps: 180, steps per second: 35, episode reward: -1490.977, mean reward: -8.283 [-100.002, 100.000], mean action: 7.900 [0.000, 12.000]\n",
      " 24209/30000: episode: 201, duration: 0.837s, episode steps: 22, steps per second: 26, episode reward: -525.987, mean reward: -23.908 [-100.001, 100.000], mean action: 5.500 [1.000, 12.000]\n",
      " 24411/30000: episode: 202, duration: 5.344s, episode steps: 202, steps per second: 38, episode reward: -1656.784, mean reward: -8.202 [-100.003, 100.000], mean action: 7.896 [1.000, 12.000]\n",
      " 24636/30000: episode: 203, duration: 6.464s, episode steps: 225, steps per second: 35, episode reward: -1472.079, mean reward: -6.543 [-100.003, 100.000], mean action: 8.062 [1.000, 12.000]\n",
      " 24695/30000: episode: 204, duration: 1.621s, episode steps: 59, steps per second: 36, episode reward: -1402.385, mean reward: -23.769 [-100.002, 100.000], mean action: 6.864 [1.000, 12.000]\n",
      " 24826/30000: episode: 205, duration: 3.440s, episode steps: 131, steps per second: 38, episode reward: -1646.682, mean reward: -12.570 [-100.001, 100.000], mean action: 7.771 [0.000, 12.000]\n",
      " 24867/30000: episode: 206, duration: 1.357s, episode steps: 41, steps per second: 30, episode reward: -237.984, mean reward: -5.804 [-100.000, 100.000], mean action: 6.805 [1.000, 12.000]\n",
      " 24895/30000: episode: 207, duration: 0.899s, episode steps: 28, steps per second: 31, episode reward: -351.286, mean reward: -12.546 [-100.000, 100.000], mean action: 6.786 [1.000, 12.000]\n",
      " 25147/30000: episode: 208, duration: 9.314s, episode steps: 252, steps per second: 27, episode reward: -2539.373, mean reward: -10.077 [-100.002, 100.000], mean action: 7.734 [0.000, 12.000]\n",
      " 25231/30000: episode: 209, duration: 2.772s, episode steps: 84, steps per second: 30, episode reward: -449.987, mean reward: -5.357 [-100.001, 100.000], mean action: 7.381 [1.000, 12.000]\n",
      " 25688/30000: episode: 210, duration: 12.721s, episode steps: 457, steps per second: 36, episode reward: -4571.770, mean reward: -10.004 [-100.003, 100.000], mean action: 7.912 [0.000, 12.000]\n",
      " 26135/30000: episode: 211, duration: 13.081s, episode steps: 447, steps per second: 34, episode reward: -3676.578, mean reward: -8.225 [-100.003, 100.000], mean action: 7.933 [0.000, 12.000]\n",
      " 26285/30000: episode: 212, duration: 4.322s, episode steps: 150, steps per second: 35, episode reward: -1137.379, mean reward: -7.583 [-100.000, 100.000], mean action: 7.800 [1.000, 12.000]\n",
      " 26644/30000: episode: 213, duration: 14.226s, episode steps: 359, steps per second: 25, episode reward: -3662.879, mean reward: -10.203 [-100.003, 100.000], mean action: 8.201 [0.000, 12.000]\n",
      " 26697/30000: episode: 214, duration: 3.123s, episode steps: 53, steps per second: 17, episode reward: -163.785, mean reward: -3.090 [-99.997, 100.000], mean action: 6.340 [1.000, 12.000]\n",
      " 26925/30000: episode: 215, duration: 10.890s, episode steps: 228, steps per second: 21, episode reward: -2473.278, mean reward: -10.848 [-100.003, 100.000], mean action: 7.838 [0.000, 12.000]\n",
      " 27645/30000: episode: 216, duration: 38.028s, episode steps: 720, steps per second: 19, episode reward: -5012.659, mean reward: -6.962 [-100.003, 100.000], mean action: 8.479 [0.000, 12.000]\n",
      " 27714/30000: episode: 217, duration: 3.635s, episode steps: 69, steps per second: 19, episode reward: -1329.880, mean reward: -19.274 [-100.003, 100.000], mean action: 7.058 [1.000, 12.000]\n",
      " 27759/30000: episode: 218, duration: 2.482s, episode steps: 45, steps per second: 18, episode reward: -572.385, mean reward: -12.720 [-100.001, 100.000], mean action: 7.044 [0.000, 12.000]\n",
      " 27962/30000: episode: 219, duration: 10.065s, episode steps: 203, steps per second: 20, episode reward: -3501.076, mean reward: -17.247 [-100.003, 100.000], mean action: 7.261 [0.000, 12.000]\n",
      " 28444/30000: episode: 220, duration: 21.571s, episode steps: 482, steps per second: 22, episode reward: -2433.366, mean reward: -5.048 [-100.003, 100.000], mean action: 8.620 [0.000, 12.000]\n",
      " 28547/30000: episode: 221, duration: 5.235s, episode steps: 103, steps per second: 20, episode reward: -751.579, mean reward: -7.297 [-100.001, 100.000], mean action: 7.961 [1.000, 12.000]\n",
      " 28801/30000: episode: 222, duration: 10.489s, episode steps: 254, steps per second: 24, episode reward: -1435.782, mean reward: -5.653 [-100.001, 100.000], mean action: 8.606 [0.000, 12.000]\n",
      " 29330/30000: episode: 223, duration: 21.691s, episode steps: 529, steps per second: 24, episode reward: -4162.777, mean reward: -7.869 [-100.003, 100.000], mean action: 8.819 [0.000, 12.000]\n",
      " 29547/30000: episode: 224, duration: 9.334s, episode steps: 217, steps per second: 23, episode reward: -1217.581, mean reward: -5.611 [-100.001, 100.000], mean action: 8.562 [0.000, 12.000]\n",
      "Training took 1257.423261 seconds\n",
      "done, took 1257.423 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    TRAINING_MODE = True\n",
    "    FILE_NAME = os.path.basename('result').split('.')[0]\n",
    "\n",
    "    # todo : need to substitute it with env.make() and also remove other parameters such as protobuf_name & verbose!?\n",
    "    # Create an Environment\n",
    "    env = AvoidReavers( move_angle=30, move_dist=3, frames_per_step=24, no_gui=True, local_speed=0)\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Create your model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(70, input_dim=STATE_SIZE, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(40, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='softmax'))\n",
    "    model.summary()\n",
    "\n",
    "    # Create your Agent\n",
    "    agent = ReinforceAgent(STATE_SIZE, action_size, processor=ReaverProcessor(), model=model,\n",
    "                           discount_factor=0.9)\n",
    "\n",
    "    agent.compile(Adam(lr=LEARNING_RATE))\n",
    "\n",
    "    # For the Graph\n",
    "    cb_plot = DrawTrainMovingAvgPlotCallback('' + FILE_NAME + '.png', 5, 5, l_label=['episode_reward'])\n",
    "    \n",
    "    \n",
    "    #agent.load_weights(os.path.realpath('../../../../../' + 'result' + '.h5f'))\n",
    "\n",
    "    # Run your agent\n",
    "    agent.run(env, 30000, train_mode=TRAINING_MODE, verbose=2, callbacks=[cb_plot])\n",
    "\n",
    "    agent.save_weights(FILE_NAME + '.h5f', False)\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
