{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Copyright (C) 2019 SAMSUNG SDS <Team.SAIDA@gmail.com>\n",
    "#\n",
    "# This code is distribued under the terms and conditions from the MIT License (MIT).\n",
    "#\n",
    "# Authors : Uk Jo, Iljoo Yoon, Hyunjae Lee, Daehun Jun\n",
    "\n",
    "# Initial framework taken from https://github.com/jaara/AI-blog/blob/master/CartPole-A3C.py\n",
    "\n",
    "from core.algorithm.PPO import PPOAgent\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from core.common.processor import Processor\n",
    "from saida_gym.starcraft.avoidReavers import AvoidReavers\n",
    "from core.callbacks import DrawTrainMovingAvgPlotCallback\n",
    "import saida_gym.envs.conn.connection_env as Config\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "\n",
    "import argparse\n",
    "from core.common.util import OPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''parser = argparse.ArgumentParser(description='PPO Configuration for Avoid_Reaver')\n",
    "\n",
    "parser.add_argument(OPS.NO_GUI.value, help='gui', type=bool, default=False)\n",
    "parser.add_argument(OPS.MOVE_ANG.value, help='move angle', default=10, type=int)\n",
    "parser.add_argument(OPS.MOVE_ANG.value, help='move dist', default=2, type=int)\n",
    "parser.add_argument(OPS.GAMMA.value, help='gamma', default=0.99, type=float)\n",
    "parser.add_argument(OPS.EPOCHS.value, help='Epochs', default=10, type=int)\n",
    "#args = parser.parse_args()\n",
    "#dict_args = vars(args)\n",
    "dict_args = {OPS.NO_GUI.value:False, OPS.MOVE_ANG.value:10, OPS.MOVE_ANG.value:2, OPS.GAMMA.value:.99, OPS.EPOCHS.value:10}\n",
    "\n",
    "post_fix = ''\n",
    "for k in dict_args.keys():\n",
    "    if k == OPS.NO_GUI():\n",
    "        continue\n",
    "    post_fix += '_' + k + '_' + str(dict_args[k])\n",
    "'''\n",
    "# Hyper param\n",
    "NO_GUI = False #dict_args[OPS.NO_GUI()]\n",
    "NB_STEPS = 50000\n",
    "STATE_SIZE = 8 + 3 * 8\n",
    "LOSS_CLIPPING = 0.2  # Only implemented clipping for the surrogate loss, paper said it was best\n",
    "EPOCHS = 10 #dict_args[OPS.EPOCHS()]\n",
    "NOISE = 0.1  # Exploration noise\n",
    "GAMMA = .99 #dict_args[OPS.GAMMA()]\n",
    "BUFFER_SIZE = 256\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_SIZE = 80\n",
    "NUM_LAYERS = 3\n",
    "ENTROPY_LOSS = 1e-3\n",
    "LR = 1e-4  # Lower lr stabilises training greatly\n",
    "\n",
    "\n",
    "def scale_velocity(v):\n",
    "    return v\n",
    "\n",
    "\n",
    "def scale_angle(angle):\n",
    "    return (angle - math.pi) / math.pi\n",
    "\n",
    "\n",
    "def scale_pos(pos):\n",
    "    return pos / 16\n",
    "\n",
    "\n",
    "def scale_pos2(pos):\n",
    "    return pos / 8\n",
    "\n",
    "\n",
    "def exponential_average(old, new, b1):\n",
    "    return old * b1 + (1-b1) * new\n",
    "\n",
    "\n",
    "# Reshape the reward in a way you want\n",
    "def reward_reshape(reward):\n",
    "    \"\"\" Reshape the reward\n",
    "        Starcraft Env returns the reward according to following conditions.\n",
    "        1. Invalid action : -0.1\n",
    "        2. get hit : -1\n",
    "        3. goal : 1\n",
    "        4. others : 0\n",
    "\n",
    "    # Argument\n",
    "        reward (float): The observed reward after executing the action\n",
    "\n",
    "    # Returns\n",
    "        reshaped reward\n",
    "    \"\"\"\n",
    "\n",
    "    if math.fabs(reward + 0.1) < 0.01:\n",
    "        reward = -1\n",
    "    elif reward == -1:\n",
    "        reward = -10\n",
    "    elif reward == 1:\n",
    "        reward = 10\n",
    "    elif reward == 0:\n",
    "        reward = -0.1\n",
    "\n",
    "    return reward\n",
    "\n",
    "\n",
    "class ReaverProcessor(Processor):\n",
    "    def __init__(self):\n",
    "        self.last_action = None\n",
    "        self.success_cnt = 0\n",
    "        self.cumulate_reward = 0\n",
    "\n",
    "    def process_action(self, action):\n",
    "        self.last_action = action\n",
    "        return action\n",
    "\n",
    "    def process_step(self, observation, reward, done, info):\n",
    "        state_array = self.process_observation(observation)\n",
    "        reward = reward_reshape(reward)\n",
    "        self.cumulate_reward += reward\n",
    "\n",
    "        if reward == 10:\n",
    "            if self.cumulate_reward > 0:\n",
    "                self.success_cnt += 1\n",
    "\n",
    "            self.cumulate_reward = 0\n",
    "            print(\"success_cnt = \", self.success_cnt)\n",
    "\n",
    "        return state_array, reward, done, info\n",
    "\n",
    "    def process_observation(self, observation, **kwargs):\n",
    "        print(observation)\n",
    "        \"\"\" Pre-process observation\n",
    "\n",
    "        # Argument\n",
    "            observation (object): The current observation from the environment.\n",
    "\n",
    "        # Returns\n",
    "            processed observation\n",
    "\n",
    "        \"\"\"\n",
    "        if len(observation.my_unit) > 0:\n",
    "            s = np.zeros(STATE_SIZE)\n",
    "            me = observation.my_unit[0]\n",
    "            # Observation for Dropship\n",
    "            s[0] = scale_pos2(me.pos_x)  # X of coordinates\n",
    "            s[1] = scale_pos2(me.pos_y)  # Y of coordinates\n",
    "            s[2] = scale_pos2(me.pos_x - 320)  # relative X of coordinates from goal\n",
    "            s[3] = scale_pos2(me.pos_y - 320)  # relative Y of coordinates from goal\n",
    "            s[4] = scale_velocity(me.velocity_x)  # X of velocity\n",
    "            s[5] = scale_velocity(me.velocity_y)  # y of coordinates\n",
    "            s[6] = scale_angle(me.angle)  # Angle of head of dropship\n",
    "            s[7] = 1 if me.accelerating else 0  # True if Dropship is accelerating\n",
    "\n",
    "            # Observation for Reavers\n",
    "            for ind, ob in enumerate(observation.en_unit):\n",
    "                s[ind * 8 + 8] = scale_pos2(ob.pos_x - me.pos_x)  # X of relative coordinates\n",
    "                s[ind * 8 + 9] = scale_pos2(ob.pos_y - me.pos_y)  # Y of relative coordinates\n",
    "                s[ind * 8 + 10] = scale_pos2(ob.pos_x - 320)  # X of relative coordinates\n",
    "                s[ind * 8 + 11] = scale_pos2(ob.pos_y - 320)  # Y of relative coordinates\n",
    "                s[ind * 8 + 12] = scale_velocity(ob.velocity_x)  # X of velocity\n",
    "                s[ind * 8 + 13] = scale_velocity(ob.velocity_y)  # Y of velocity\n",
    "                s[ind * 8 + 14] = scale_angle(ob.angle)  # Angle of head of Reavers\n",
    "                s[ind * 8 + 15] = 1 if ob.accelerating else 0  # True if Reaver is accelerating\n",
    "\n",
    "        return s\n",
    "\n",
    "\n",
    "def build_actor(state_size, action_size, advantage, old_prediction):\n",
    "    state_input = Input(shape=(state_size,))\n",
    "\n",
    "    x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "    for _ in range(NUM_LAYERS - 1):\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "    out_actions = Dense(action_size, activation='softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_actor_continuous(state_size, action_size, advantage, old_prediction):\n",
    "\n",
    "    state_input = Input(shape=(state_size,))\n",
    "    x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "\n",
    "    for _ in range(NUM_LAYERS - 1):\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "    out_actions = Dense(action_size, name='output', activation='tanh')(x)\n",
    "\n",
    "    model = Model(inputs=[state_input, advantage, old_prediction], outputs=[out_actions])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_critic(state_size):\n",
    "\n",
    "    state_input = Input(shape=(state_size,))\n",
    "    x = Dense(HIDDEN_SIZE, activation='tanh')(state_input)\n",
    "    for _ in range(NUM_LAYERS - 1):\n",
    "        x = Dense(HIDDEN_SIZE, activation='tanh')(x)\n",
    "\n",
    "    out_value = Dense(1)(x)\n",
    "\n",
    "    model = Model(inputs=[state_input], outputs=[out_value])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize...\n",
      "Shared Memory create\u0000\n",
      "SAIDA_AR16420 Shared memory found.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: Tensor(\"add:0\", shape=(?, 2), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-3007c8c2ab31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontinuous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_actor_continuous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTATE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mADVANTAGE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOLD_PREDICTION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_actor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTATE_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mACTION_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mADVANTAGE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOLD_PREDICTION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8c0e7d7ce403>\u001b[0m in \u001b[0;36mbuild_actor_continuous\u001b[1;34m(state_size, action_size, advantage, old_prediction)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[0mout_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m160\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m160\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madvantage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_prediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_actions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# Graph network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# Subclassed network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m    186\u001b[0m                                  \u001b[1;34m'the output of a Keras `Layer` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                                  \u001b[1;34m'(thus holding past layer metadata). '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m                                  'Found: ' + str(x))\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         self._compute_previous_mask = (\n",
      "\u001b[1;31mValueError\u001b[0m: Output tensors to a Model must be the output of a Keras `Layer` (thus holding past layer metadata). Found: Tensor(\"add:0\", shape=(?, 2), dtype=float32)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    training_mode = True\n",
    "    load_model = False\n",
    "    FILE_NAME = os.path.basename('').split('.')[0] + \"-\" + datetime.now().strftime(\"%m%d%H%M%S\")\n",
    "    action_type = 1\n",
    "\n",
    "    env = AvoidReavers(move_angle=10, move_dist=2, frames_per_step=16\n",
    "                       , verbose=0, action_type=action_type, no_gui=NO_GUI)\n",
    "\n",
    "    ACTION_SIZE = env.action_space.n\n",
    "\n",
    "    continuous = False if action_type == 0 else True\n",
    "\n",
    "    # Build models\n",
    "    actor = None\n",
    "    ADVANTAGE = Input(shape=(1,))\n",
    "    OLD_PREDICTION = Input(shape=(ACTION_SIZE,))\n",
    "\n",
    "    if continuous:\n",
    "        actor = build_actor_continuous(STATE_SIZE, ACTION_SIZE, ADVANTAGE, OLD_PREDICTION)\n",
    "    else:\n",
    "        actor = build_actor(STATE_SIZE, ACTION_SIZE, ADVANTAGE, OLD_PREDICTION)\n",
    "\n",
    "    critic = build_critic(STATE_SIZE)\n",
    "\n",
    "    agent = PPOAgent(STATE_SIZE, ACTION_SIZE, continuous, actor, critic, GAMMA, LOSS_CLIPPING, EPOCHS, NOISE, ENTROPY_LOSS,\n",
    "                     BUFFER_SIZE,BATCH_SIZE, processor=ReaverProcessor())\n",
    "\n",
    "    agent.compile(optimizer=[Adam(lr=LR), Adam(lr=LR)], metrics=[ADVANTAGE, OLD_PREDICTION])\n",
    "\n",
    "    cb_plot = DrawTrainMovingAvgPlotCallback(os.path.realpath(''), 10, 5, l_label=['episode_reward'])\n",
    "\n",
    "    agent.run(env, NB_STEPS, train_mode=training_mode, verbose=2, callbacks=[cb_plot], action_repetition=1, nb_episodes=1000)\n",
    "\n",
    "    if training_mode:\n",
    "        agent.save_weights(os.path.realpath(\"../../save_model\"),\"avoid_Reavers_PPO\")\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
