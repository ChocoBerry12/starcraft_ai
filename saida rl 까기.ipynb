{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 코드 까기\n",
    "---\n",
    "## 1. Agent\n",
    "> `python/core/common/agent.py`\n",
    "\n",
    "* 이 클래스는 커스텀 agent 구현할 때 base 가 되는 클래스로, 반드시 implement 한다. 그리고 아래 함수들을 재정의한다. 더 세부적인 부분을 셋팅할 수도 있다.\n",
    "* 이 클라스는 Env 와 상호작용한다. agent 는 action 을 전달하고 Env 는 observation 과 reward 를 전달해준다. \n",
    "* 학습 알고리즘을 정의하는 곳이다. 학습법을 테스트하려면 이 곳을 커스텀하여 재정의한다.\n",
    "\n",
    "### Agent class\n",
    " * `forward(observation) -> action` - network 에 input 넣어서 action output 얻는 부분\n",
    " * `backward(reward, terminal) -> metrics` - 학습을 위한 back prop 정의 부분 - metrics 반환\n",
    " * `compile(optimizer, metrics)` - metrics 와 optimizer 를 이용해 network learning 그래프 정의\n",
    " * `load_weights`\n",
    " * `save_weights`\n",
    "\n",
    "---\n",
    "## 2. Processor\n",
    "> `python/core/common/processor.py`\n",
    "\n",
    "* 이 클래스는 Agent - Env 사이에 데이터를 교환해주는 역할을 맡는다. 즉 데이터를 자유롭게 조작하고 설계할 수 있는 곳이다.\n",
    "* Agent 는 학습 알고리즘을 커스텀하는 곳이라면 Processor 는 데이터를 커스텀하는 곳이다.\n",
    "* 그래서 processor 클래스를 implement 해서 사용한다.\n",
    "* 전체구조는 `Agent <-> Processor <-> Env` 형태다.\n",
    "\n",
    "### Processor class\n",
    "* `process_step(self, observation, reward, done, info) -> observation, reward, done, reward` - state, reward 등을 재정의해서 반환\n",
    "* `process_observation(observation) -> observation` - observation 을 재정의. raw 데이터는 json 형태라서 parsing 을 해야함. example 참조\n",
    "* `process_reward(reward) -> reward` - reward 를 재정의\n",
    "* `process_info(info) -> info` - info(flag 등 특정 정보) 를 재정의. hierachy 등의 고급 RL 에 쓰임이 있을 듯. dictionary 형태\n",
    "* `process_action(action) -> action` - action 재정의. network 출력을 scaling 하는 등의 작업을 여기서 해도 될듯.\n",
    "\n",
    "---\n",
    "## 3. 나머지\n",
    "* processor 객체는 Agent 에게 반드시 넘겨준다.\n",
    "* nn model 객체도 만들어서 Agent 에게 넘겨주어야 한다.\n",
    "* `agent.compile` 함수는 nn 학습을 직접 돌리는 함수.\n",
    "* 중간중간 이벤트에 콜백함수를 정의해 그래프 그리기 등의 작업가능. 디버깅이나 체크할 때 유용할 듯.\n",
    "* `agent.run` 함수를 호출하면 모든것이 시작됨. 학습, 시뮬레이션\n",
    "\n",
    "---\n",
    "## 4. Issue & 해결\n",
    "* avoidReaver 예제는 discrete action space 에서 돌아가는데 continuous action space 에서 어떻게 해야하는지 알 수 없었음.\n",
    "* avoidObserver 예제는 continuous action space 기반으로 작성됨. \n",
    "* 코드를 깐 결과, `Processor` 클래스의 `process_action` 메서드에서 action 을 커스텀 해줘야한다는 결론을 얻음.\n",
    "* 이 예제에서는 `theta`, `radian`, `action_num` 의 3 가지의 action 리스트가 필요한데, radian 과 action_num 은 하드코딩되어있고, 각도에 해당하는 theta 값이 continuous 한 값.\n",
    "* 즉, action 도출에 관련있는 policy network 의 출력은 크기가 1 인 float 타입이다. \n",
    "* 튜토리얼에 있는 내용에 알맞게 커스텀 하면 문제가 없다.\n",
    "\n",
    "## 5. 할 것\n",
    "* avoidReaver 환경에서 action reshape 한 후 action_type == 1, 2 인 모드 실행하기\n",
    "* agent 클래스, processor 클래스 성공적으로 커스텀 한 후 학습시키기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
